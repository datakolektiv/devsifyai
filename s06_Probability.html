<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Session06 Probability Theory</title>

<script src="site_libs/header-attrs-2.29/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.13.2/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html"><img id="logo" style="width: 30px;" src="DK_Logo_White_NoTitle_25.png" /></a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Sessions
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="s00_installations.html">S00. R and Python Installations</a>
    </li>
    <li>
      <a href="s01_introduction_to_r.html">S01. Introduction to R</a>
    </li>
    <li>
      <a href="s02_dataframe.html">S02. I/O, Packages, and data.frame</a>
    </li>
    <li>
      <a href="s03_functional.html">S03. Functional programming + Control flow in R</a>
    </li>
    <li>
      <a href="s04_more_functional.html">S04. More functional Programming + Basic Statistics in R</a>
    </li>
    <li>
      <a href="s05_EDA.html">S05. Exploratory Data Analysis (EDA)</a>
    </li>
    <li class="dropdown-header">S06. Probability Theory 01</li>
    <li>
      <a href="s06_Probability.html"></a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Labs
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="l01_lists.html">L01. Data Types, Lists</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Notes
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="n00_installations.html">N00. R and Python Installations</a>
    </li>
    <li>
      <a href="n03_functional.html">N03. Functional Programming</a>
    </li>
    <li>
      <a href="n04_more_functional.html">N04. More Functional Programming</a>
    </li>
  </ul>
</li>
<li>
  <a href="video.html">Video</a>
</li>
<li>
  <a href="resources.html">Resources</a>
</li>
<li>
  <a href="about.html">About</a>
</li>
<li>
  <a href="media.html">Media</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Session06 Probability Theory</h1>
<h4 class="author">Goran S. Milovanovic, PhD, Aleksandar Cvetković,
Phd</h4>

</div>


<p><img src="DK_Logo_100.png" /></p>
<hr />
<div id="probability-theory" class="section level1">
<h1>Probability Theory</h1>
<div
id="authors-aleksandar-cvetković-phd-mathematics-goran-s.-milovanović-phd-psychology"
class="section level2">
<h2>Authors: Aleksandar Cvetković, Phd, Mathematics, Goran S.
Milovanović, Phd, Psychology</h2>
<hr />
<div id="what-do-we-want-to-do-today" class="section level3">
<h3>What do we want to do today?</h3>
<p>This is the beginning of our journey into Data Science and Analytics
in R. Thus far, we have been learning about R programming, more or less
treating R as any other programming language. Now we begin to master the
real power of R in its areas of specialization: analytics, statistics,
machine learning, visualization, and reporting. And as you will see: in
statistics, nothing compares to R.</p>
<p>Exploratory Data Analysis (EDA) is a concept in Data
Analytics/Statistics promoted by John Tukey in the 60s who defined
<strong>data analysis</strong> as:</p>
</div>
</div>
<div id="setup" class="section level2">
<h2>0. Setup</h2>
<pre class="r"><code>library(tidyverse)</code></pre>
</div>
<div id="event-and-experimental-probability" class="section level2">
<h2>1. Event and Experimental Probability</h2>
<p>When asked: <em>What’s the probability of landing Tails when tossing
a (fair) coin?</em> You’d (probably) answer: <span
class="math inline">\(\frac{1}{2}\)</span>. Or 50%. But what does that
mean?</p>
<p>And when asked: <em>What’s the probability of getting 6 when rolling
a six-sided (fair) die?</em> The expected answer: <span
class="math inline">\(\frac{1}{6}\)</span>. <em>What about NOT getting
6?</em> It’s: <span class="math inline">\(\frac{5}{6}\)</span>. And
<em>getting a number less than five?</em> <span
class="math inline">\(\frac{2}{3}\)</span>? But what do those numbers
mean?</p>
<p>And and - <em>The probability of drawing an Ace from a (fair) deck of
cards?</em></p>
<hr />
<p>Let’s get back to the coin. <span
class="math inline">\(\frac{1}{2}\)</span> says you. But what does it
mean? Maybe you meant that out of two tossings you’ll land exactly one
Tails (<span class="math inline">\(TH\)</span> or <span
class="math inline">\(HT\)</span>)? Okay, let’s toss a coin two times. I
got <span class="math inline">\(TT\)</span>. Maybe my assumption was
wrong? Or I’ve just got unlucky? Let’s toss it again. <span
class="math inline">\(HT\)</span> this time. Maybe I’m actually right?
Or I got lucky. Another two tosses! <span
class="math inline">\(HH\)</span>. Hmm… Unlucky again? But can I talk
about luck in mathematics and in hypothesis testing?</p>
<p>What about the other answer: 50%. Does that mean that I’ll land Tails
exactly half the times when tossing a coin <span
class="math inline">\(N\)</span> times? Let’s toss it, say, 10 times.
But I don’t actually have a coin. Luckily I have R which can simulate
tossing a coin.</p>
<pre class="r"><code># - init random number generator: set.seed()
set.seed(42) 

# - a coin
coin &lt;- c(&#39;H&#39;, &#39;T&#39;)

# - a sample: numerical simulation of 10 coin tosses
tossing = sample(coin, size=10, replace = TRUE)

# - result
tossing</code></pre>
<pre><code>##  [1] &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot;</code></pre>
<p>5 of 10 Tails. Exactly 50%! But, wait. Let’s toss the coin again!</p>
<pre class="r"><code>tossing = sample(coin, size=10, replace = TRUE)
tossing</code></pre>
<pre><code>##  [1] &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot;</code></pre>
<p>Hm, 6 out of 10 Tails. Ok now is this a fair coin or not?</p>
<p>Let’s toss a coin 100 times then.</p>
<pre class="r"><code>tossing = sample(coin, size=100, replace = TRUE)
tossing</code></pre>
<pre><code>##   [1] &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot;
##  [24] &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot;
##  [47] &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;H&quot;
##  [70] &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot;
##  [93] &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot;</code></pre>
<p>How many Tails, then?</p>
<pre class="r"><code>table(tossing)</code></pre>
<pre><code>## tossing
##  H  T 
## 44 56</code></pre>
<p>56/100. Not bad. But not quite 50%. Lucky, but not as quite? (But can
I talk about luck in mathematics and in hypothesis testing?) What if
those 50% is actually an <em>approximation</em> of the result? Let’s
toss a coin 1000 times.</p>
<pre class="r"><code>tossing = sample(coin, size=1000, replace = TRUE)
table(tossing)</code></pre>
<pre><code>## tossing
##   H   T 
## 501 499</code></pre>
<p>499/1000. That’s <em>almost there</em>. And that’s not that quite bad
<em>approximation</em>. But still… Now, let’s toss a coin ONE MILLION
TIMES.</p>
<pre class="r"><code>tossing = sample(coin, size=10^6, replace = TRUE)
table(tossing)</code></pre>
<pre><code>## tossing
##      H      T 
## 499601 500399</code></pre>
<p>And that would be 0.499601 - not bad an approximation…</p>
<p>What if tossed a coin more than million times?</p>
<pre class="r"><code>tossing = sample(coin, size=10^7, replace = TRUE)
table(tossing)</code></pre>
<pre><code>## tossing
##       H       T 
## 4999541 5000459</code></pre>
<p>Now it’s 0.4999541 - even close to 50:50. This is even better,
veeeery close to 50%.</p>
<p>And if we tossed a coin <strong>infinitely many times</strong>? Then
there is no approximation. We’d get 50% sharp. Mathematically, we can
write this</p>
<p><span class="math display">\[ P(T) = \lim_{N_{\rm
TRIES}\rightarrow\infty}\frac{n_{\rm HITS}}{N_{\rm TRIES}} = 0.5 =
\frac{1}{2}.\]</span></p>
<p>What are all these letters? Let’s interpret this formula one by
one.</p>
<ul>
<li><p><span class="math inline">\(T\)</span> is the <em>event</em>:
event of landing Tails when tossing ONE coin.</p></li>
<li><p><span class="math inline">\(P\)</span> is the
<em>probability</em>: we may consider it as a function which
<em>measures</em> how probable an event is. So, <span
class="math inline">\(P(T)\)</span> is probability of landing Tails when
tossing one coin.</p></li>
<li><p><span class="math inline">\(N_{\rm TRIES}\)</span> is the number
of trials of the SAME experiment. In our case, it’s the number of
tossing a coin.</p></li>
<li><p><span class="math inline">\(n_{\rm HITS}\)</span> is the number
of ‘hits’, i.e. the number of desired outcomes in our set of trials. In
our case, it’s the number of Tails landed.</p></li>
<li><p><span class="math inline">\(\lim_{N_{\rm
TRIES}\rightarrow\infty}\)</span> is the <em>limit</em>: a value of the
fraction <span class="math inline">\(\frac{n_{\rm HITS}}{N_{\rm
TRIES}}\)</span> which we would obtain if we would
<em>theoretically</em> be able to toss a coin infinitely many times in
our finite lives. But we can’t do that. We can only obtain an
<em>experimental</em> (or <em>statistical</em>) approximation of the
<em>theoretical</em> value in finite number of trials. And as we
demonstrated - the larger number of trials, the better the
approximation.</p></li>
</ul>
<hr />
<p>Let’s now illustrate this <em>limit</em> on another example: rolling
a six-sided die. Intuitively we say that the probability of getting a 6
is <span class="math inline">\(\frac{1}{6}\)</span>, or 16.666…% .
Similarly as with the coin, we’ll simulate rolling a die for various
numbers of trials and list the results, i.e. approximations of
theoretical probability.</p>
<pre class="r"><code># - number_of_rolls:
number_of_rolls &lt;- round(10^seq(1.75, 7, by = 0.0525))
print(number_of_rolls)</code></pre>
<pre><code>##   [1]       56       63       72       81       91      103      116      131      148      167
##  [11]      188      213      240      271      305      345      389      439      495      559
##  [21]      631      712      804      907     1023     1155     1303     1471     1660     1873
##  [31]     2113     2385     2692     3037     3428     3868     4365     4926     5559     6273
##  [41]     7079     7989     9016    10174    11482    12957    14622    16501    18621    21014
##  [51]    23714    26761    30200    34080    38459    43401    48978    55271    62373    70388
##  [61]    79433    89640   101158   114156   128825   145378   164059   185140   208930   235776
##  [71]   266073   300262   338844   382384   431519   486968   549541   620155   699842   789769
##  [81]   891251  1005773  1135011  1280855  1445440  1631173  1840772  2077304  2344229  2645453
##  [91]  2985383  3368992  3801894  4290422  4841724  5463865  6165950  6958250  7852356  8861352
## [101] 10000000</code></pre>
<pre class="r"><code># - die &lt;- 1:6
die &lt;- 1:6

# - experiment
exp_prob &lt;- lapply(number_of_rolls, function(x) {
  
  rolls &lt;- sample(die, x, replace = TRUE)
  result &lt;- table(rolls)
  number_of_sixes = result[6]
  p_of_six &lt;- number_of_sixes/x
  return(
    data.frame(NumberOfRolls = x,
               NumberOfSixes = number_of_sixes,
               P_Six = p_of_six)
    )
})

# - put the result together
exp_prob_df &lt;- Reduce(rbind, exp_prob)

# - Show me:
print(exp_prob_df)</code></pre>
<pre><code>##      NumberOfRolls NumberOfSixes      P_Six
## 6               56            11 0.19642857
## 61              63             9 0.14285714
## 62              72             6 0.08333333
## 63              81            12 0.14814815
## 64              91            11 0.12087912
## 65             103            15 0.14563107
## 66             116            17 0.14655172
## 67             131            23 0.17557252
## 68             148            28 0.18918919
## 69             167            25 0.14970060
## 610            188            33 0.17553191
## 611            213            36 0.16901408
## 612            240            30 0.12500000
## 613            271            53 0.19557196
## 614            305            47 0.15409836
## 615            345            59 0.17101449
## 616            389            59 0.15167095
## 617            439            67 0.15261959
## 618            495            81 0.16363636
## 619            559            95 0.16994633
## 620            631            98 0.15530903
## 621            712           115 0.16151685
## 622            804           134 0.16666667
## 623            907           159 0.17530320
## 624           1023           172 0.16813294
## 625           1155           189 0.16363636
## 626           1303           196 0.15042210
## 627           1471           252 0.17131203
## 628           1660           283 0.17048193
## 629           1873           305 0.16284036
## 630           2113           358 0.16942735
## 631           2385           379 0.15890985
## 632           2692           439 0.16307578
## 633           3037           534 0.17583141
## 634           3428           550 0.16044341
## 635           3868           616 0.15925543
## 636           4365           684 0.15670103
## 637           4926           772 0.15671945
## 638           5559           916 0.16477784
## 639           6273          1045 0.16658696
## 640           7079          1185 0.16739652
## 641           7989          1359 0.17010890
## 642           9016          1519 0.16847826
## 643          10174          1661 0.16325929
## 644          11482          1907 0.16608605
## 645          12957          2141 0.16523887
## 646          14622          2408 0.16468335
## 647          16501          2746 0.16641416
## 648          18621          3073 0.16502873
## 649          21014          3458 0.16455696
## 650          23714          3934 0.16589356
## 651          26761          4500 0.16815515
## 652          30200          4988 0.16516556
## 653          34080          5607 0.16452465
## 654          38459          6414 0.16677501
## 655          43401          7394 0.17036474
## 656          48978          8098 0.16533954
## 657          55271          9146 0.16547557
## 658          62373         10542 0.16901544
## 659          70388         11678 0.16590896
## 660          79433         13351 0.16807876
## 661          89640         14932 0.16657742
## 662         101158         16794 0.16601752
## 663         114156         19052 0.16689443
## 664         128825         21672 0.16822822
## 665         145378         24186 0.16636630
## 666         164059         27257 0.16614145
## 667         185140         30972 0.16728962
## 668         208930         34860 0.16685014
## 669         235776         39252 0.16648005
## 670         266073         44816 0.16843498
## 671         300262         50432 0.16795998
## 672         338844         56601 0.16704147
## 673         382384         63811 0.16687675
## 674         431519         71723 0.16621053
## 675         486968         81423 0.16720401
## 676         549541         91448 0.16640797
## 677         620155        103411 0.16675025
## 678         699842        116675 0.16671620
## 679         789769        131520 0.16652971
## 680         891251        149045 0.16723123
## 681        1005773        168412 0.16744534
## 682        1135011        189407 0.16687680
## 683        1280855        212946 0.16625301
## 684        1445440        241303 0.16694086
## 685        1631173        271963 0.16672848
## 686        1840772        307180 0.16687564
## 687        2077304        345675 0.16640559
## 688        2344229        391063 0.16681945
## 689        2645453        441318 0.16682133
## 690        2985383        497125 0.16651967
## 691        3368992        562046 0.16682913
## 692        3801894        634396 0.16686315
## 693        4290422        715802 0.16683720
## 694        4841724        806347 0.16654130
## 695        5463865        911138 0.16675705
## 696        6165950       1027054 0.16656866
## 697        6958250       1157696 0.16637747
## 698        7852356       1306201 0.16634511
## 699        8861352       1476417 0.16661306
## 6100      10000000       1668185 0.16681850</code></pre>
<p>Let’s plot the results</p>
<pre class="r"><code>ggplot(exp_prob_df, 
       aes(x = log(NumberOfRolls),
           y = P_Six)) + 
  geom_line(color = &quot;darkblue&quot;, size = .25) +
  geom_point(fill = &quot;white&quot;, color = &quot;darkblue&quot;, size = 1) +
  xlab(&quot;Number of rolls&quot;) + 
  ylab(&quot;Number of Sixes&quot;) + 
  ggtitle(&quot;Statistical Experiment: \nRolling a fair die and counting the number of sixes&quot;) +
  theme_minimal() + 
  theme(plot.title = element_text(hjust = .5, size = 10))</code></pre>
<p><img src="s06_Probability_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>From the scatterplot above we see how the values of ratio <span
class="math inline">\(\frac{n_{\rm HITS}}{N_{\rm TRIES}}\)</span>
converge towards the predicted theoretical probability of 16.666…% as
<span class="math inline">\(N_{\rm TRIES}\rightarrow\infty\)</span>.</p>
<p>But, is there a way to calculate theoretical probability
<strong>exactly</strong>?</p>
</div>
<div id="sigma-algebra-of-events-and-theoretical-probability"
class="section level2">
<h2>2. <span class="math inline">\(\sigma\)</span>-Algebra of Events and
Theoretical Probability</h2>
<p>In order to be able to speak properly about theoretical probability
(and be able to compute it), we need to introduce <em><span
class="math inline">\(\sigma\)</span>-algebra of events</em> and
<em>probability-as-a-measure</em>. We can consider <span
class="math inline">\(\sigma\)</span>-algebra of events as a family of
sets where every set is an event, and every element of this event-set is
an <em>outcome</em> for which consider the evenet realized, i.e. a
<em>favorable outcome</em>.</p>
<p>For example, if we toss a coin two times, then one event-set might
be</p>
<p><span class="math display">\[ A - {\rm Landed\ 1\ Heads\ and\ 1\
Tails}, \]</span></p>
<p>and its elements are</p>
<p><span class="math display">\[ A = \{HT, TH\}.\]</span></p>
<p>A set of all the possible outcomes for a given experiment/observation
is called the <em>universal set</em>. It is denoted by <span
class="math inline">\(\Omega\)</span> and it is the set upon which <span
class="math inline">\(\sigma\)</span>-algebra of events is built upon,
from the subsets <span class="math inline">\(A\subset\Omega\)</span> of
the universal set.</p>
<p>For tossing a coin two times we have</p>
<p><span class="math display">\[ \Omega = \{HH, HT, TH,
TT\},\]</span></p>
<p>and obviously</p>
<p><span class="math display">\[ A \subset \Omega. \]</span></p>
<p><span class="math display">\[1^\circ\quad \emptyset,\ \Omega \in
\Sigma\]</span></p>
<p><span class="math display">\[2^\circ\quad A^C \in \Sigma\]</span></p>
<p><span class="math display">\[3^\circ\quad A\cup B \in
\Sigma\]</span></p>
<p><span class="math display">\[4^\circ\quad A\cap B \in
\Sigma.\]</span></p>
<p>What do these cryptic messages even mean? Let’s explain them one by
one, I promise they make sense.</p>
<p><span class="math display">\[1^\circ\quad \emptyset,\ \Omega \in
\Sigma\]</span></p>
<p>This means that the empty set <span
class="math inline">\(\emptyset\)</span> and the total set <span
class="math inline">\(\Omega\)</span> are also considered as events.</p>
<p><span class="math inline">\(\emptyset\)</span> is called an
<em>impossible event</em>, and it does not contain any (possible)
outcome.</p>
<p><span class="math inline">\(\Omega\)</span>, viewed as an event is
called <em>certain event</em> - getting any outcome from all the
possible outcomes is definitely a certain event.</p>
<hr />
<p><span class="math display">\[2^\circ\quad A^C \in \Sigma\]</span></p>
<p>An complementary set of <span class="math inline">\(A\)</span>: <span
class="math inline">\(A^C = \Omega\setminus A\)</span> is also
considered as an event. Every unfavourable outcome of <span
class="math inline">\(A\)</span> is favourable for <span
class="math inline">\(A^C\)</span> (and vice versa). So, for the event
<span class="math inline">\(A\)</span> as defined above, we have:</p>
<p><span class="math display">\[ A^C = \{HH, TT\}. \]</span></p>
<hr />
<p><span class="math display">\[3^\circ\quad A\cup B \in
\Sigma\]</span></p>
<p>A union of two events (denoted also as <span class="math inline">\(A
+ B\)</span>) is also considered as an event. But what is a union of two
events actually? We say that the event <span class="math inline">\(A\cup
B\)</span> is realized when at least one of the events <span
class="math inline">\(A\)</span> <strong>OR</strong> <span
class="math inline">\(B\)</span> is realized. <span
class="math inline">\(A\cup B\)</span> contains all the outcomes which
are favourable for either the event <span
class="math inline">\(A\)</span> <strong>OR</strong> event <span
class="math inline">\(B\)</span>.</p>
<p>For example, if we define event <span
class="math inline">\(B\)</span> as</p>
<p><span class="math display">\[B - {\rm Landed\ 2\ Tails,}\]</span></p>
<p>we have</p>
<p><span class="math display">\[ A\cup B = \{HT, TH, TT\}.\]</span></p>
<hr />
<p><span class="math display">\[4^\circ\quad A\cap B \in
\Sigma.\]</span></p>
<p>An intersection of two events (denoted also as AB) is also considered
as an event. We say that the event <span
class="math inline">\(AB\)</span> is realized when both events <span
class="math inline">\(A\)</span> <strong>AND</strong> <span
class="math inline">\(B\)</span> are simultaneously realized. <span
class="math inline">\(AB\)</span> contains all the outcomes which are
favourable for both events <span class="math inline">\(A\)</span>
<strong>AND</strong> <span class="math inline">\(B\)</span>.</p>
<p>For examle, if we define event <span class="math inline">\(C\)</span>
as</p>
<p><span class="math display">\[ C - {\rm Tails\ in\ the\ first\ coin\
toss}, \]</span></p>
<p>we have</p>
<p><span class="math display">\[ AC = \{HT, TH\} \cap \{TH, TT\} =
\{TH\}.\]</span></p>
<p>Two events <span class="math inline">\(A\)</span> and <span
class="math inline">\(B\)</span> are <em>mutually exclusive</em> or
<em>disjoint</em> if <span class="math inline">\(A\cap B =
\emptyset\)</span>, i.e. if realization of both events simultaneously is
an impossible event.</p>
<p>Two events <span class="math inline">\(A\)</span> and <span
class="math inline">\(B\)</span> are <em>independant</em> if the outcome
of one event does not influence the outcome of the other. For example,
when tossing a coin two times, the result of the first toss does not
influence the outcome of the second.</p>
<hr />
<p>One more importan notion is the <em>elementary event</em> - an event
containing a single possible outcome. So, for tossing our coin two
times, elementary events are: <span class="math inline">\(\{HH\},\
\{HT\},\ \{TH\}\)</span> and <span
class="math inline">\(\{TT\}\)</span>.</p>
<p><span class="math inline">\(\sigma\)</span>-algebras of events serve
us as a brige between the natural language by which we describe events
and their outcomes with formal mathematical language to describe them
via sets, their elements and set operations. The upside of mathematical
objects is that it is natural to impose some measure on them, and now we
can measure the events, i.e. measure the probability of event
realization. So we can define probability-as-a-measure, that is a
mapping <span class="math inline">\(P\)</span> from <span
class="math inline">\(\sigma\)</span>-algebra of events to interval a
set of real numbers via following set of <em>axioms</em> (known as
<em>Kolmogorov Axioms</em>):</p>
<p><strong>Axiom 1</strong>: For any event <span
class="math inline">\(A\)</span> we have</p>
<p><span class="math display">\[ P(A)\geqslant 0.\]</span></p>
<p><strong>Axiom 2</strong>: For certain event <span
class="math inline">\(\Omega\)</span> we have</p>
<p><span class="math display">\[ P(\Omega) = 1.\]</span></p>
<p><strong>Axiom 3</strong>: For mutually exclusive events <span
class="math inline">\(A_1, A_2, \ldots, A_n, \ldots\)</span> we have</p>
<p><span class="math display">\[P\Big(\bigcup_{i=1}^{\infty}A_i\Big) =
\sum_{i=1}^{\infty}P(A_i).\]</span></p>
<p>What do those axioms tell us?</p>
<p><strong>Axiom 1</strong> means that the probability of any event has
to be some non-negative real number; in other words - we cannot have
negative probability (the same way we cannot have negative length,
surface or volume - which are also measures of some kind)</p>
<p><strong>Axiom 2</strong> says that the probability of a certain event
is 1 (or 100%). As we know that <span
class="math inline">\(\Omega\)</span>, viewed as a set, is a universal
set, i.e. set that contains all the possible outcomes - this axiom also
tells us that the probability of observing any outcome of all possible
defined outcomes is equal to 1.</p>
<p>If we want to compute a probability of some union of mutually
exclusive events, <strong>Axiom 3</strong> tells us that we can do that
just by summing the probabilities of every single event.</p>
<hr />
<p>And here’s a nice reminder for <span
class="math inline">\(\sigma\)</span>-algebras and Kolmogorov
Axioms:</p>
<p><img src="_img/prob.png" /> These axioms have very usefull
consequences; if <span class="math inline">\(A\)</span> and <span
class="math inline">\(B\)</span> are two events, we have:</p>
<p><span class="math display">\[ 1^\circ\ P(\emptyset) = 0,
\]</span></p>
<p><span class="math display">\[ 2^\circ\ 0 \leqslant P(A) \leqslant 1,
\]</span></p>
<p><span class="math display">\[ 3^\circ\ P(A^C) = 1 - P(A),
\]</span></p>
<p><span class="math display">\[ 4^\circ\ A\subseteq B \Rightarrow P(A)
\leqslant P(B).\]</span></p>
<p><span class="math display">\[ 5^\circ\ A,\ B\ {\rm indep.}
\Rightarrow P(AB) = P(A)P(B).\]</span></p>
<p>Let’s now go over these consequences.</p>
<p><span class="math display">\[ 1^\circ\ P(\emptyset) = 0 \]</span></p>
<p>We saw that impossible event is represented by <span
class="math inline">\(\emptyset\)</span>. So, this tells us that the
probability of an impossible event is 0.</p>
<hr />
<p><span class="math display">\[ 2^\circ\ 0 \leqslant P(A) \leqslant 1
\]</span></p>
<p>Not only that the probability of an event is some non-negative real
number - it’s a real number belonging to the interval [0, 1]; and the
endpoints of this interval correspond to the impossible event (<span
class="math inline">\(P(\emptyset) = 0\)</span>) and certain event
(<span class="math inline">\(P(\Omega) = 1\)</span>). The closer the
probability is to 1, the more probable is the realization of an event.
This also alows us to speak about probabilities in terms of
percents.</p>
<hr />
<p><span class="math display">\[ 3^\circ\ P(A^C) = 1 - P(A)
\]</span></p>
<p>This tells us how to simply calculate probability of a complementary
event. If a probability of an event happening is 22%, then the
probability of it NOT happening is 78%.</p>
<hr />
<p><span class="math display">\[4^\circ\ A\subseteq B \Rightarrow P(A)
\leqslant P(B) \]</span></p>
<p>If set <span class="math inline">\(A\)</span> is contained in set
<span class="math inline">\(B\)</span>, i.e. if all the outcomes
favourable for event <span class="math inline">\(A\)</span> are also
favourable for event <span class="math inline">\(B\)</span>, then event
<span class="math inline">\(A\)</span> has smaller (or equal) chances
for realization than the event <span class="math inline">\(B\)</span>.
In other words - events with smaller set of favourable outcomes are less
probable.</p>
<hr />
<p><span class="math display">\[ 5^\circ\ A,\ B\ {\rm indep.}
\Rightarrow P(AB) = P(A)P(B).\]</span></p>
<p>If two events are independant, then the probability of them both
occuring is equal to the product of probabilites of each event occuring
independantly. If we toss a coin two times, we can calculate</p>
<p><span class="math display">\[P(HT) = P(H)P(T).\]</span></p>
<hr />
<p>All this talk about the Probability Theory, but we still haven’t
figured out how to calculate theoretical probability. Don’t worry, we
are almost there - and we have all the ingredients to write a formula
that stems quite naturally from the theoretical foundations above.</p>
<p>As we saw, probabilty of an event should be some number between 0 and
1, with probabilities of impossible and certain event as extreme values.
And the bigger the event/set is, the biger its probability should be.
This leads us to define theoretical probability of an event (<span
class="math inline">\(A\)</span>) via the following simple formula:</p>
<p><span class="math display">\[P(A) = \frac{|A|}{|\Omega|} = \frac{{\rm
No.\ of\ all\ the\ favourable\ outcomes\ for}\ A}{{\rm No.\ of\ all\
the\ possible\ outcomes}}.\]</span></p>
<p>(<span class="math inline">\(|A|\)</span> is the <em>cardinality</em>
of a set, i.e. a number of elements that set has.)</p>
<p>One can easily check that formula for the probability, as given
above, satisfies all the Kolmogorov Axioms and, of course, all the
listed consequences.</p>
<hr />
<p>Now that we have the ‘formula for probability’ we can easily
calculate probabilities of the events listed at the beginning of this
notebook.</p>
<ul>
<li>Probability of landing Tails on a coin toss:</li>
</ul>
<p><span class="math display">\[P(T) = \frac{|\{T\}|}{|\{H, T\}|} =
\frac{1}{2}.\]</span></p>
<ul>
<li>Probability of landing at least one Tails on two coin tosses (event
<span class="math inline">\(A\)</span>):</li>
</ul>
<p><span class="math display">\[P(A) = \frac{|\{TH, HT, TT\}|}{|\{HH,
TH, HT, TT\}|} = \frac{3}{4}.\]</span></p>
<ul>
<li>Probability of getting 6 when rolling a six-sided die:</li>
</ul>
<p><span class="math display">\[P(X = 6) = \frac{|\{6\}|}{|\{1, 2, 3, 4,
5, 6\}|} = \frac{1}{6}.\]</span></p>
<ul>
<li>Probability of getting less than 5 when rolling a six-sided
die:</li>
</ul>
<p><span class="math display">\[P(X &lt; 5) = \frac{|\{1, 2, 3,
4\}|}{|\{1, 2, 3, 4, 5, 6\}|} = \frac{4}{6} = \frac{2}{3}.\]</span></p>
<ul>
<li>Probability of getting any number from 1 to 6 when rolling a
six-sided die:</li>
</ul>
<p><span class="math display">\[P(1\leqslant X\leqslant 6) = \frac{|\{1,
2, 3, 4, 5, 6\}|}{|\{1, 2, 3, 4, 5, 6\}|} = \frac{6}{6} =
1.\]</span></p>
<ul>
<li>Probability of getting a 7 when rolling a six-sided die:</li>
</ul>
<p><span class="math display">\[P(X = 7) = \frac{|\emptyset|}{|\{1, 2,
3, 4, 5, 6\}|} = \frac{0}{6} = 0.\]</span></p>
<ul>
<li>Probability of getting 6 or 7 when rolling a 20-sided die:</li>
</ul>
<p><span class="math display">\[P(\{X=6\}\cup\{X=7\}) = P(X=6) + P(X=7)
= \frac{1}{20} + \frac{1}{20} = \frac{2}{20} =
\frac{1}{10}.\]</span></p>
<ul>
<li>Probability of getting less than 19 when rolling a 20-sided
die:</li>
</ul>
<p><span class="math display">\[P(X &lt; 19) = 1 - P(\{X &lt; 19\}^C) =
1 - P(X\geqslant 19) =  1 -\frac{|\{19, 20\}|}{|\{1, 2, \ldots, 20\}|} =
1 - \frac{2}{20} = \frac{18}{20} = \frac{9}{10}.\]</span></p>
<ul>
<li>Probability of getting two sixes in two dice rolls:</li>
</ul>
<p><span class="math display">\[P(\{X_1 = 6\}\cap\{X_2 = 6\}) = P(\{X_1
= 6\})P(\{X_2 = 6\}) = \frac{1}{6}\cdot\frac{1}{6} =
\frac{1}{36}.\]</span></p>
<ul>
<li>Probability of drawing an Ace from a deck of cards:</li>
</ul>
<pre class="r"><code>cat(&#39;P(X = A) = |{\U0001F0A1, \U0001F0B1, \U0001F0C1, \U0001F0D1}|/|Whole Deck of Cards| = 4/52 = 1/13.&#39;)</code></pre>
<pre><code>## P(X = A) = |{🂡, 🂱, 🃁, 🃑}|/|Whole Deck of Cards| = 4/52 = 1/13.</code></pre>
<p>Even though we have a tool to calculate probability of an event
exactly, we shouldn’t forget about experimental probability. First, it
can serve us to experimentally check our theoretical calculation.
Secondly, and more important: sometimes calculating theoretical
probability is difficult, or even impossible; so, performing the
experiments and noting down the results is a way to obtain the
probability of an event.</p>
<p>While we’ve been using the given formula for calculating theoretical
probabilities, there was actually one important thing that we hid under
the rug - the formula works only in case when <span
class="math inline">\(\Omega\)</span> is finite set. But what if <span
class="math inline">\(\Omega\)</span> is infinite? Does all this
mathematical construction breaks down? Actually no. <span
class="math inline">\(\sigma\)</span>-algebras, Kolmogorov Axioms and
their consequences actually give us tools to handle even infinite <span
class="math inline">\(\Omega\)</span>s, and we’ll see how later on.</p>
<p>But first let’s talk about <em>random variables</em>.</p>
</div>
<div id="random-variable-a-discrete-type" class="section level2">
<h2>3. Random variable: a Discrete Type</h2>
<p>When we were calculating theoretical probabilities a bit above we
introduced the following notation <span class="math inline">\(P(X =
6)\)</span> or <span class="math inline">\(P(X \geqslant 19)\)</span>.
What is this <span class="math inline">\(X\)</span>? It is actually a
random variable (RV) - a varable which can take one of the several
different values, each with its own probability.</p>
<p>There are two types of random variables: <em>discrete</em> and
<em>(absolutely) continuous</em>. Values of RV are elements,
i.e. outcomes of set <span class="math inline">\(\Omega\)</span>. If
<span class="math inline">\(\Omega\)</span> is finite or <em>countably
infinite</em>, then associated RV is discrete. If <span
class="math inline">\(\Omega\)</span> is <em>uncountably infinite</em>,
then the corresponding RV is continuous.</p>
<p>We’ll speak about discrete RVs now.</p>
<hr />
<p>A discrete random variable is fully described if we know all the
values it can take, and all the probabilities corresponding to those
values. This ‘description’ of a random variable is called its
<em>distribution</em>. So, ‘knowing’ a random variable is equivalent to
‘knowing’ its distribution.</p>
<p>For tossing a coin, a random variable <span
class="math inline">\(X\)</span> can take two values: Heads and Tails,
each with probability <span class="math inline">\(\frac{1}{2}\)</span>.
We can write its distribution as</p>
<p><span class="math display">\[ X :
\begin{pmatrix}
H &amp; T\\
\frac{1}{2} &amp; \frac{1}{2}
\end{pmatrix}.\]</span></p>
<p>This ‘function’ which assigns a probability to each outcome is also
called <em>probability mass function</em> or <em>p.m.f</em>.</p>
<p>For rolling a six sided die, we have a random variable which can take
some of the values from the die given with p.m.f</p>
<p><span class="math display">\[ X :
\begin{pmatrix}
1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6\\
\frac{1}{6} &amp; \frac{1}{6} &amp; \frac{1}{6} &amp; \frac{1}{6} &amp;
\frac{1}{6} &amp; \frac{1}{6}
\end{pmatrix}.\]</span></p>
<p>We can even define RVs on our own. The only important thing is that
all the probabilites in its distribution need to sum to 1 (as the union
of all elementary events, i.e. of every possible outcome gives <span
class="math inline">\(\Omega\)</span>, and <span
class="math inline">\(P(\Omega) = 1\)</span>).</p>
<p>For example:</p>
<p><span class="math display">\[ X :
\begin{pmatrix}
-1 &amp; 0 &amp; 2 &amp; 4.7\\
0.2 &amp; 0.05 &amp; 0.45 &amp; 0.3
\end{pmatrix}.\]</span></p>
<p>We can simulate the outcomes of this RV using
<code>sample()</code>:</p>
<pre class="r"><code>choices &lt;- c(-1, 0, 2, 4.6)
sample(choices, size = 1, prob = c(.2, .05, .45, .3))</code></pre>
<pre><code>## [1] 0</code></pre>
<pre class="r"><code>choices &lt;- c(-1, 0, 2, 4.6)
result &lt;- sample(choices, size = 100, prob = c(.2, .05, .45, .3), replace = TRUE)
result</code></pre>
<pre><code>##   [1]  2.0  2.0  2.0 -1.0 -1.0 -1.0  2.0  2.0  2.0  2.0 -1.0  4.6  2.0  2.0  4.6  2.0  2.0  4.6 -1.0
##  [20]  4.6  4.6 -1.0  2.0  4.6  4.6 -1.0  4.6  4.6  2.0  4.6 -1.0  4.6  4.6  2.0  2.0  4.6  0.0  0.0
##  [39] -1.0 -1.0 -1.0  4.6  4.6  4.6  4.6 -1.0 -1.0  0.0  0.0 -1.0  2.0  2.0  2.0 -1.0  4.6  2.0  4.6
##  [58]  2.0  2.0 -1.0 -1.0  4.6  4.6  2.0  4.6  4.6  4.6 -1.0  4.6  0.0  2.0  2.0  4.6  4.6  2.0  2.0
##  [77]  2.0  2.0  2.0 -1.0  2.0  2.0  2.0  4.6  4.6  4.6  2.0 -1.0 -1.0  0.0  4.6  4.6  4.6  2.0  2.0
##  [96]  2.0  2.0 -1.0  2.0  2.0</code></pre>
<pre class="r"><code>result_frame &lt;- as.data.frame(table(result))
result_frame</code></pre>
<pre><code>##   result Freq
## 1     -1   22
## 2      0    6
## 3      2   39
## 4    4.6   33</code></pre>
<pre class="r"><code>ggplot(result_frame,
       aes(x = result,
           y = Freq, 
           color = result,
           fill = result)) + 
  geom_bar(stat = &quot;identity&quot;) + 
  xlab(&quot;Values&quot;) + ylab(&quot;Frequency&quot;) +
  theme_minimal() + 
  ggtitle(&quot;Statistical Experiment\nSome discrete distribution that I have made up&quot;) +
  theme(plot.title = element_text(hjust = .5, size = 10)) +
  theme(legend.position = &quot;none&quot;)</code></pre>
<p><img src="s06_Probability_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<hr />
<p>Let’s assume we have a following discrete random variable which can
take infinitely many values:</p>
<p><span class="math display">\[ X :
\begin{pmatrix}
1 &amp; 2 &amp; \cdots &amp; k &amp; \cdots \\
p_1 &amp; p_2 &amp; \cdots &amp; p_k &amp; \cdots &amp;
\end{pmatrix}.\]</span></p>
<p>Using its distribution, we can calculate probabilities of some events
in the following manner (which also applies for finite RVs):</p>
<p><span class="math display">\[ P(X = k) = p_k \]</span></p>
<p><span class="math display">\[ P(X \leqslant k) = p_1 + p_2 + \cdots +
p_k \]</span></p>
<p><span class="math display">\[ P(X &lt; k) = P(X \leqslant k - 1) =
p_1 + p_2 + \cdots + p_{k-1} \]</span></p>
<p><span class="math display">\[ P(3 \leqslant X \leqslant k) = p_3 +
p_4 + \cdots + p_k \]</span></p>
<p><span class="math display">\[ P(X &gt; k) = 1 - P(X \leqslant k) = 1
- (p_1 + p_2 + \cdots + p_k) \]</span></p>
<hr />
<p>Probability <span class="math inline">\(P(X\leqslant k)\)</span> is
quite an important one - it even has it’s name <em>cumulative
distribution function</em> or <em>c.d.f.</em>. It’s denoted by <span
class="math inline">\(F(k)\)</span>, and it simply sums all the
probabilities for all values up to <span
class="math inline">\(k\)</span>:</p>
<p><span class="math display">\[ F :
\begin{pmatrix}
1 &amp; 2 &amp;  3 &amp; \cdots &amp; N\\
p_1 &amp; p_1 + p_2  &amp; p_1 + p_2 + p_3 &amp; \cdots &amp; 1
\end{pmatrix}.\]</span></p>
</div>
<div id="discrete-type-distributions" class="section level2">
<h2>4. Discrete Type Distributions</h2>
<p>Random or <em>stochastic</em> processes are not just some
mathematical intellectual plaything. They occur all around us and within
us. Many stochastic natural and social phenomena behave according to
some distribution, and can be modelled according to that distribution.
Here we go through some of those important discrete-type
distributions.</p>
<div id="bernoulli-distribution" class="section level3">
<h3>Bernoulli Distribution</h3>
<p>The simplest discrete distribution is Bernoulli Distribution - it’s a
distribution of a RV that has binary outcomes - yes/no, hit/miss or
success/failure. The <em>parameter</em> of this distribution is <span
class="math inline">\(p\)</span> - the probability of ‘hit’. This
distribution looks like:</p>
<p><span class="math display">\[ X :
\begin{pmatrix}
0 &amp; 1\\
1 - p &amp; p
\end{pmatrix}.\]</span></p>
<p>To sample from Bernoulli Distribution we can use
<code>sample()</code> with two outcomes. Say, we have a population that
gives a ‘yes’ answer to some question in 2/3 cases and ‘no’ in 1/3
cases, and we want to draw a sample of 100 from it. Here’s a sample:</p>
<pre class="r"><code>X = c(&#39;yes&#39;, &#39;no&#39;)
no_tries = 100
outcomes = sample(X, prob = c(2/3, 1/3), size = no_tries, replace = TRUE)
no_outcomes = as.data.frame(table(outcomes))
ggplot(no_outcomes,
       aes(x = outcomes,
           y = Freq, 
           color = outcomes,
           fill = outcomes)) + 
  geom_bar(stat = &quot;identity&quot;) + 
  xlab(&quot;Outcomes&quot;) + ylab(&quot;Frequency&quot;) +
  theme_minimal() + 
  ggtitle(&quot;Bernoulli&#39;s Statistical Experiment&quot;) +
  theme(plot.title = element_text(hjust = .5, size = 10)) +
  theme(legend.position = &quot;none&quot;)</code></pre>
<p><img src="s06_Probability_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
</div>
<div id="binomial-distribution" class="section level3">
<h3>Binomial Distribution</h3>
<p>Binomial distribution gives an answer to the following question:
<em>what’s the probability of getting <span
class="math inline">\(k\)</span> hits in <span
class="math inline">\(n\)</span> trials of the same hit/miss experiment,
having probability of hit equal to <span
class="math inline">\(p\)</span>?</em></p>
<p>Binomial Distribution has two parameters:</p>
<ul>
<li><span class="math inline">\(n\)</span>: number of repetitions of the
same experiment</li>
<li><span class="math inline">\(p\)</span>: probability of a hit
(success)</li>
</ul>
<p>If a random variable <span class="math inline">\(X\)</span> has a
Binomial Distribution with parameters <span
class="math inline">\(n\)</span> and <span
class="math inline">\(p\)</span>, we write that</p>
<p><span class="math display">\[ X \sim \mathcal{B}(n, p).\]</span></p>
<p>For example, we’d like to know what’s the probability of landing 7
Heads in tossing a coin 10 times. We can approximate this probability by
sampling from the Binomial Distribution with parameters <span
class="math inline">\(n=10\)</span> and <span
class="math inline">\(p=0.5\)</span> (<span
class="math inline">\(\mathcal{B}(100, 0.5)\)</span>), using
<code>rbinom()</code>.</p>
<pre class="r"><code>outcomes = rbinom(n = 100, prob = .5, size = 10)
outcomes</code></pre>
<pre><code>##   [1] 7 5 4 6 4 3 6 6 8 5 7 6 7 4 5 8 0 9 5 5 6 7 4 5 6 6 3 5 5 4 7 4 9 4 6 5 4 4 2 8 3 8 7 3 1 4 4
##  [48] 5 7 7 6 5 4 6 5 5 5 7 2 5 7 5 5 6 4 6 4 4 4 5 6 7 7 6 8 5 3 4 4 6 6 4 6 5 6 5 7 6 7 4 6 6 7 4
##  [95] 3 5 5 5 5 6</code></pre>
<pre class="r"><code>no_outcomes = as.data.frame(table(outcomes))
ggplot(no_outcomes,
       aes(x = outcomes,
           y = Freq)) +
  geom_bar(stat = &quot;identity&quot;, color = &quot;darkblue&quot;, fill = &quot;white&quot;) + 
  xlab(&quot;Outcomes&quot;) + ylab(&quot;Frequency&quot;) +
  theme_minimal() + 
  ggtitle(&quot;Binomial Statistical Experiment&quot;) +
  theme(plot.title = element_text(hjust = .5, size = 10)) +
  theme(legend.position = &quot;none&quot;)</code></pre>
<p><img src="s06_Probability_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>We can actually calculate theoretical Binomial probabilities, via</p>
<p><span class="math display">\[P(X = k) =
\binom{n}{k}p^k(1-p)^{n-k},\]</span></p>
<p>where <span class="math inline">\(\binom{n}{k}\)</span>, called
<em>binomial coefficient</em>, is</p>
<p><span class="math display">\[\binom{n}{k} =
\frac{n!}{k!(n-k)!}.\]</span></p>
<p>Say we have some unfair coin which lands Tails with probability 0.25,
and we toss it three times. Using the formula above for <span
class="math inline">\(\mathcal{B}(3, 0.25)\)</span> we obtain the
probabilities for number of Tails in 3 tosses <span
class="math inline">\(X\)</span>:</p>
<p><span class="math display">\[ X :
\begin{pmatrix}
0 &amp; 1 &amp; 2 &amp; 3\\
0.421875 &amp; 0.421875 &amp; 0.140625 &amp; 0.015625
\end{pmatrix}.\]</span></p>
</div>
<div id="interlude-functions-to-work-with-probability-in-r"
class="section level3">
<h3>Interlude: Functions to work with probability in R</h3>
<p>Consider the following experiment: a person rolls a fair dice ten
times. <strong>Q.</strong> What is the probability of obtaining five or
less sixes at random?</p>
<p>We know that R’s <code>dbinom()</code> represents the binomial
probability mass function (p.m.f.). Let’s see: the probability of
getting <em>exactly</em> five sixes at random is:</p>
<pre class="r"><code>pFiveSixes &lt;- dbinom(5, size = 10, p = 1/6)
pFiveSixes</code></pre>
<pre><code>## [1] 0.01302381</code></pre>
<p>Do not be confused by our attempt to model dice rolls by a binomial
distribution: in fact, there are only two outcomes here, “6 is obtained”
with <span class="math inline">\(p=1/6\)</span> and “everything else”
with <span class="math inline">\(1−p=5/6\)</span>!</p>
<p>Then, the probability of getting five or less than five sixes from
ten statistical experiments is:</p>
<pre class="r"><code>pFiveAndLessSixes &lt;- sum(
  dbinom(0, size = 10, p = 1/6),
  dbinom(1, size = 10, p = 1/6),
  dbinom(2, size = 10, p = 1/6),
  dbinom(3, size = 10, p = 1/6),
  dbinom(4, size = 10, p = 1/6),
  dbinom(5, size = 10, p = 1/6)
)
pFiveAndLessSixes</code></pre>
<pre><code>## [1] 0.9975618</code></pre>
<p>in order to remind ourselves that the probabilities of all outcomes
from a discrete probability distribution - in our case, that “0 sixes”,
“1 six”, “2 sixes”, “3 sixes”, “4 sixes”, or “5 sixes” etc. obtain -
will eventually sum up to one. However, let’s wrap this up elegantly by
using sapply()</p>
<pre class="r"><code>pFiveAndLessSixes &lt;- sum(sapply(seq(0,5), function(x) {
  dbinom(x, size = 10, p = 1/6)
}))
pFiveAndLessSixes</code></pre>
<pre><code>## [1] 0.9975618</code></pre>
<p>or, even better, by recalling that we are working with a vectorized
programming language:</p>
<pre class="r"><code>pFiveAndLessSixes &lt;- sum(dbinom(seq(0,5), size = 10, p =1/6))
pFiveAndLessSixes</code></pre>
<pre><code>## [1] 0.9975618</code></pre>
<p>Of course, we could have used the <strong>cumulative distribution
function</strong>* (c.d.f) to figure out this as well:</p>
<pre class="r"><code>pFiveAndLessSixes &lt;- pbinom(5, size = 10, p = 1/6)
pFiveAndLessSixes</code></pre>
<pre><code>## [1] 0.9975618</code></pre>
<div id="random-number-generation-from-the-binomial"
class="section level4">
<h4>Random Number Generation from the Binomial</h4>
<p><code>rbinom()</code> will provide a vector of random deviates from
the Binomial distribution with the desired parameter, e.g.:</p>
<pre class="r"><code># Generate a sample of random binomial variates:
randomBinomials &lt;- rbinom(n = 100, size = 1, p = .5)
randomBinomials</code></pre>
<pre><code>##   [1] 1 0 1 0 0 0 1 0 1 0 0 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 0 0 0 1 0 0 1 0 0
##  [48] 1 0 0 1 0 0 0 0 1 1 0 0 1 1 0 0 0 1 1 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 1 1 1 0 0 0 1 1 0 0 0 1
##  [95] 0 1 1 0 0 1</code></pre>
<p>Now, if each experiment encompasses 100 coin tosses:</p>
<pre class="r"><code>randomBinomials &lt;- rbinom(n = 100, size = 100, p = .5)
randomBinomials # see the difference?</code></pre>
<pre><code>##   [1] 46 52 50 42 53 51 49 43 45 49 57 52 61 61 58 49 51 49 42 46 45 48 48 49 50 48 53 49 52 50 46
##  [32] 55 49 50 53 50 52 48 51 45 48 55 61 49 53 53 46 42 56 57 47 50 46 51 42 48 60 46 53 51 48 46
##  [63] 47 50 44 49 60 51 46 46 49 47 51 47 40 51 48 48 49 55 50 48 53 48 50 54 53 49 43 65 48 51 45
##  [94] 50 53 47 44 51 44 50</code></pre>
<pre class="r"><code>randomBinomials &lt;- rbinom(n = 100, size = 10000, p = .5)
randomBinomials</code></pre>
<pre><code>##   [1] 4935 5020 5015 5064 5007 4988 4949 5010 5042 4924 5041 5021 5000 4992 4922 5058 4969 4888 5105
##  [20] 4960 5038 5020 4923 5028 4920 4977 4997 4972 4955 5039 5104 5002 5024 4937 4986 5063 5038 4974
##  [39] 4968 4984 4981 5064 4972 5004 5037 5055 4950 4952 4996 4991 5089 5054 4981 4985 5000 4939 5032
##  [58] 5010 4989 4998 4976 5058 5140 5008 4929 5057 4972 4968 4923 5053 4975 4983 5043 4991 5053 5022
##  [77] 5084 5052 5028 4941 5072 5052 5001 4999 5008 5038 4979 5050 4959 5053 4956 5043 5059 5043 5099
##  [96] 4971 4955 4967 5023 5052</code></pre>
<p>Let’s plot the distribution of the previous experiment:</p>
<pre class="r"><code>randomBinomialsPlot &lt;- data.frame(success = randomBinomials)
ggplot(randomBinomialsPlot, 
       aes(x = success)) + 
  geom_histogram(binwidth = 10, 
                 fill = &#39;white&#39;, 
                 color = &#39;darkblue&#39;) +
  theme_minimal() + 
  theme(panel.border = element_blank())</code></pre>
<p><img src="s06_Probability_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<p>Interpretation: we were running 100 statistical experiments, each
time drawing a sample of 10000 observations of a fair coin (<span
class="math inline">\(p=.5\)</span>). And now,</p>
<pre class="r"><code>randomBinomials &lt;- rbinom(100000, size = 100000, p = .5)
randomBinomialsPlot &lt;- data.frame(success = randomBinomials)
ggplot(randomBinomialsPlot, 
       aes(x = success)) + 
  geom_histogram(binwidth = 10, 
                 fill = &#39;white&#39;, 
                 color = &#39;darkblue&#39;) +
  theme_minimal() + 
  theme(panel.border = element_blank())</code></pre>
<p><img src="s06_Probability_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<p>… we were running 10000 statistical experiments, each time drawing a
sample of 100000 observations of a fair coin (<span
class="math inline">\(p=.5\)</span>).</p>
<p>So, we have the Probability Mass Function (p.m.f):</p>
<pre class="r"><code>heads &lt;- 0:100
binomialProbability &lt;- dbinom(heads, size = 100, p = .5)
sum(binomialProbability)</code></pre>
<pre><code>## [1] 1</code></pre>
<p>Where <code>sum(binomialProbability) == 1</code> is <code>TRUE</code>
because this is a discrete distribution so its Probability Mass
Functions outputs probability indeed!</p>
<pre class="r"><code>binomialProbability &lt;- data.frame(heads = heads,
                                  density = binomialProbability)
ggplot(binomialProbability, 
       aes(x = heads, 
           y = density)) + 
  geom_bar(stat = &quot;identity&quot;, 
           fill = &#39;white&#39;,
           color = &#39;darkblue&#39;) +
  ggtitle(&quot;Binomial P.M.F.&quot;) +
  theme_minimal() + 
  theme(panel.border = element_blank()) +
  theme(plot.title = element_text(hjust = .5, size = 10))</code></pre>
<p><img src="s06_Probability_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<p>The Cumulative Distribution Function (c.d.f), on the other hand:</p>
<pre class="r"><code>heads &lt;- 1:100
binomialProbability &lt;- pbinom(heads, size = 100, p = .5)
sum(binomialProbability)</code></pre>
<pre><code>## [1] 51</code></pre>
<pre class="r"><code>binomialProbability &lt;- data.frame(heads = heads,
                                  cumprob = binomialProbability)
ggplot(binomialProbability, 
       aes(x = heads, 
           y = cumprob)) + 
  geom_bar(stat = &quot;identity&quot;, 
           fill = &#39;white&#39;,
           color = &#39;darkblue&#39;) +
  ylab(&quot;P(heads &lt;= x)&quot;) + 
  ggtitle(&quot;Binomial C.D.F.&quot;) +
  theme_minimal() + 
  theme(panel.border = element_blank()) +
  theme(plot.title = element_text(hjust = .5, size = 10))</code></pre>
<p><img src="s06_Probability_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
</div>
<div id="quantile-function-of-the-binomial-distribution"
class="section level4">
<h4>Quantile Function of the Binomial distribution</h4>
<p>The quantile is defined as the smallest value <span
class="math inline">\(x\)</span> such that <span
class="math inline">\(F(x)\geq p\)</span>, where <span
class="math inline">\(F\)</span> is the cumulative distribution function
(c.d.f.):</p>
<pre class="r"><code>qbinom(p = .01, size = 100, prob = .5)</code></pre>
<pre><code>## [1] 38</code></pre>
<pre class="r"><code>qbinom(p = .99, size = 100, prob = .5)</code></pre>
<pre><code>## [1] 62</code></pre>
<pre class="r"><code>qbinom(p = .01, size = 200, prob = .5)</code></pre>
<pre><code>## [1] 84</code></pre>
<pre class="r"><code>qbinom(p = .99, size = 200, prob = .5)</code></pre>
<pre><code>## [1] 116</code></pre>
<p>Similarly, we could have obtained <span
class="math inline">\(Q_1\)</span>, <span
class="math inline">\(Q_3\)</span>, and the median, for say n of 100
(that is <code>size</code> in <code>qbinom()</code>) and <span
class="math inline">\(p\)</span> of <span
class="math inline">\(.5\)</span> (that is <code>prob</code> in
<code>qbinom()</code>):</p>
<pre class="r"><code>qbinom(p = .25, size = 100, prob = .5)</code></pre>
<pre><code>## [1] 47</code></pre>
<pre class="r"><code>qbinom(p = .5, size = 100, prob = .5)</code></pre>
<pre><code>## [1] 50</code></pre>
<pre class="r"><code>qbinom(p = .75, size = 100, prob = .5)</code></pre>
<pre><code>## [1] 53</code></pre>
<pre class="r"><code>qbinom(p = .25, size = 1000, prob = .5)</code></pre>
<pre><code>## [1] 489</code></pre>
<p>Here are some examples of the binomial distribution in nature and
society:</p>
<ul>
<li><p>Coin Flips: When flipping a fair coin multiple times, the number
of heads obtained follows a binomial distribution. Each flip is an
independent trial with two outcomes (heads or tails) and an equal
probability of success (0.5 for a fair coin).</p></li>
<li><p>Product Quality Control: In manufacturing processes, the number
of defective items in a production batch can often be modeled using a
binomial distribution. Each item is inspected independently, and it is
classified as either defective or non-defective based on predetermined
criteria.</p></li>
<li><p>Election Outcomes: In a two-party political system, the
distribution of seats won by each party in a series of elections can be
modeled using a binomial distribution. Each election is treated as an
independent trial with two possible outcomes (win or loss) and a fixed
probability of success for each party.</p></li>
<li><p>Medical Trials: Clinical trials often involve testing the
effectiveness of a treatment or medication on a group of patients. The
binomial distribution can be used to model the number of patients who
respond positively to the treatment, where each patient is considered a
trial with a fixed probability of response.</p></li>
<li><p>Sports Performance: The success rate of an athlete in a specific
type of action, such as free throws in basketball or penalty kicks in
soccer, can be modeled using a binomial distribution. Each attempt
represents an independent trial with two outcomes (success or failure)
and a fixed probability of success for the athlete.</p></li>
<li><p>Genetic Inheritance: The transmission of certain genetic traits
from parents to offspring can be modeled using a binomial distribution.
For example, the probability of a child inheriting a specific allele
from a heterozygous parent follows a binomial distribution.</p></li>
<li><p>Survey Responses: In opinion polls or market research surveys,
the distribution of responses to a yes-or-no question can be modeled
using a binomial distribution. Each participant provides a single
response, which can be categorized as either a success (yes) or failure
(no) outcome.</p></li>
</ul>
</div>
</div>
<div id="poisson-distribution" class="section level3">
<h3>Poisson Distribution</h3>
<p><em>Poisson</em> is a discrete probability distribution with <em>mean
and variance correlated</em>. This is the distribution of the number of
occurrences of independent events in a given interval.</p>
<p>The p.m.f. is given by:</p>
<p><span class="math display">\[{P(X=k)} =
\frac{\lambda^{k}e^{-\lambda}}{k!}\]</span> where <span
class="math inline">\(\lambda\)</span> is the average number of events
per interval, and <span class="math inline">\(k = 0, 1,
2,...\)</span></p>
<p>For the Poisson distribution, we have that the mean (the expectation)
is the same as the variance:</p>
<p><span class="math display">\[X \sim Poisson(\lambda) \Rightarrow
\lambda = E(X) = Var(X) \]</span></p>
<p><em>Example.</em> (Following and adapting from Ladislaus Bortkiewicz,
1898). Assumption: on the average, 10 soldiers in the Prussian army were
killed accidentally by horse kick monthly. What is the probability that
17 or more soldiers in the Prussian army will be accidentally killed by
horse kicks during the month? Let’s see:</p>
<pre class="r"><code>tragedies &lt;- ppois(16, lambda=10, lower.tail=FALSE)   # upper tail (!)
tragedies</code></pre>
<pre><code>## [1] 0.02704161</code></pre>
<p>Similarly as we have used <code>pbinom()</code> to compute cumulative
probability from the binomial distribution, here we have used
<code>ppois()</code> for the Poisson distribution. The
<code>lower.tail=F</code> argument turned the cumulative into a
decumulative (or survivor) function: by calling
<code>ppois(17, lambda=10, lower.tail=FALSE)</code> we have asked not
for <span class="math inline">\(P(X \leq k)\)</span>, but for <span
class="math inline">\(P(X &gt; k)\)</span> instead. However, if this is
the case, than our <code>ppois(16, lambda=10)</code> answer would be
incorrect, and that is why we called:
<code>ppois(16, lambda=10, lower.tail=FALSE)</code> instead. Can you see
it? You have to be very careful about how exactly your probability
functions are defined (c.f. <code>Poisson {stats}</code> documentation
at (<a
href="https://stat.ethz.ch/R-manual/R-devel/library/stats/html/Poisson.html"
class="uri">https://stat.ethz.ch/R-manual/R-devel/library/stats/html/Poisson.html</a>)
and find out whether <code>lower.tail=T</code> implies <span
class="math inline">\(P(X &gt; k)\)</span> or <span
class="math inline">\(P(X \geq k)\)</span>).</p>
<pre class="r"><code># Compare:
tragedies &lt;- ppois(17, lambda=10, lower.tail=TRUE)   # lower tail (!)
tragedies</code></pre>
<pre><code>## [1] 0.9857224</code></pre>
<p>This ^^ is the answer to the question of what would be the
probability of 17 <em>and and less than 17</em> deaths.</p>
<p>The same logic to generate random deviates as we have observed in the
Binomial case is present here; we have <code>rpois()</code>:</p>
<pre class="r"><code>poissonDeviates &lt;- rpois(100000,lambda = 5)
poissonDeviates &lt;- data.frame(events = poissonDeviates)
ggplot(poissonDeviates, 
       aes(x = events)) + 
  geom_histogram(binwidth = 1, 
                 fill = &#39;white&#39;, 
                 color = &#39;darkorange&#39;) +
  ggtitle(&quot;Poisson Distribution&quot;) +
  theme_minimal() + 
  theme(panel.border = element_blank()) +
  theme(plot.title = element_text(hjust = .5, size = 10))</code></pre>
<p><img src="s06_Probability_files/figure-html/unnamed-chunk-43-1.png" width="672" /></p>
<p>Observe how the shape of the Poisson distribution changes with its
mean and variance, both represented as <span
class="math inline">\(\lambda\)</span>:</p>
<pre class="r"><code>lambda &lt;- 1:20
poissonDeviates &lt;- lapply(lambda, rpois, n = 1000)
poissonDeviates &lt;- reduce(poissonDeviates, rbind)
dim(poissonDeviates)</code></pre>
<pre><code>## [1]   20 1000</code></pre>
<p>Ok, then:</p>
<pre class="r"><code>poissonDeviates &lt;- as.data.frame(poissonDeviates)
poissonDeviates$id &lt;- 1:dim(poissonDeviates)[1]
poissonDeviates &lt;- poissonDeviates %&gt;% 
  pivot_longer(cols = -id) %&gt;% 
  select(-name)
poissonDeviates$id &lt;- factor(poissonDeviates$id, 
                             levels = sort(unique(poissonDeviates$id)))</code></pre>
<p>and finally:</p>
<pre class="r"><code>ggplot(poissonDeviates, 
       aes(x = value)) + 
  geom_histogram(binwidth = 1, 
                 fill = &#39;white&#39;, 
                 color = &#39;darkorange&#39;) +
  xlab(&quot;Events&quot;) + 
  ggtitle(expression(lambda)) + 
  facet_wrap(~id, scales = &quot;free_x&quot;) +
  theme_minimal() + 
  theme(panel.border = element_blank()) + 
  theme(strip.background = element_blank()) + 
  theme(strip.text =  element_text(size = 12)) +
  theme(plot.title = element_text(hjust = .5, size = 20))</code></pre>
<p><img src="s06_Probability_files/figure-html/unnamed-chunk-46-1.png" width="672" /></p>
<p>Let’s study one important property of the Poisson distribution:</p>
<pre class="r"><code>lambda &lt;- 1:100
poissonMean &lt;- sapply(lambda, function(x) {
  mean(rpois(100000,x))
})
poissonVar &lt;- sapply(lambda, function(x) {
  var(rpois(100000,x))
})
poissonProperty &lt;- data.frame(mean = poissonMean, 
                              variance = poissonVar)
ggplot(poissonProperty, 
       aes(x = mean, 
           y = variance)) + 
  geom_line(color = &quot;darkorange&quot;, size = .25) + 
  geom_point(color = &quot;darkorange&quot;, fill = &quot;darkorange&quot;, size = 1.5) + 
  xlab(&quot;E(X)&quot;) + ylab(&quot;Var(X)&quot;) + 
  ggtitle(&quot;Poisson Distribution\nThe Mean is equal to Variance&quot;) +
  theme_minimal() + 
  theme(panel.border = element_blank()) +
  theme(plot.title = element_text(hjust = .5, size = 10))</code></pre>
<p><img src="s06_Probability_files/figure-html/unnamed-chunk-47-1.png" width="672" /></p>
<p>Here are some examples of how the Poisson distribution can be
observed in various contexts:</p>
<ul>
<li><p>Natural Disasters: The occurrence of earthquakes, volcanic
eruptions, or meteorite impacts can be modeled using a Poisson
distribution. While these events are rare, they tend to follow a pattern
of random occurrence over time.</p></li>
<li><p>Traffic Accidents: The frequency of traffic accidents at a
particular intersection or along a specific road can often be
approximated by a Poisson distribution. The assumption is that accidents
happen randomly, and the number of accidents in a given time period
follows a Poisson distribution.</p></li>
<li><p>Phone Calls to a Call Center: The number of incoming phone calls
to a call center during a specific time interval can often be modeled
using a Poisson distribution. Even though call volumes may vary, they
tend to follow a pattern of random occurrence throughout the
day.</p></li>
<li><p>Radioactive Decay: The decay of radioactive particles follows a
Poisson distribution. Each particle has a fixed probability of decaying
within a given time period, and the number of decays within that time
period follows a Poisson distribution.</p></li>
<li><p>Birth and Death Rates: In population studies, the number of
births or deaths in a particular region over a fixed time period can be
approximated using a Poisson distribution. While births and deaths may
exhibit seasonal or long-term trends, the occurrences within short
intervals can be modeled as random events.</p></li>
<li><p>Email Arrival: The arrival of emails in an inbox can often be
modeled using a Poisson distribution. The assumption is that emails
arrive randomly, and the number of emails received within a fixed time
period can follow a Poisson distribution.</p></li>
<li><p>Molecular Collision Events: In physics and chemistry, the number
of collision events between molecules within a given volume can be
described using a Poisson distribution. This applies to scenarios such
as gas diffusion or the movement of particles in a fluid.</p></li>
<li><p>Network Traffic: The number of packets arriving at a network
router or the number of requests received by a web server within a
specific time interval can often be modeled using a Poisson
distribution. This is particularly useful in analyzing network
congestion and capacity planning.</p></li>
<li><p>Defective Products: The number of defective products in a
manufacturing process can be modeled using a Poisson distribution. It
helps assess the quality control measures and estimate the likelihood of
encountering defective items in a production batch.</p></li>
<li><p>Hospital Admissions: The number of patients admitted to a
hospital within a given time period can often be approximated by a
Poisson distribution. This information aids in resource allocation and
staffing decisions.</p></li>
<li><p>Rare Disease Occurrence: The occurrence of rare diseases within a
population can be modeled using a Poisson distribution. This
distribution helps estimate the likelihood of rare diseases and evaluate
the effectiveness of preventive measures or treatments.</p></li>
<li><p>Natural Birth Intervals: The time intervals between births in
animal populations, such as a particular species of birds, can follow a
Poisson distribution. This helps researchers understand reproductive
patterns and population dynamics.</p></li>
<li><p>Insurance Claims: The number of insurance claims filed within a
specific time period, such as automobile accidents or property damage
claims, can often be modeled using a Poisson distribution. This assists
insurers in risk assessment and pricing policies.</p></li>
<li><p>Arrival of Customers: The arrival of customers at a retail store
or the number of customers in a queue at a bank can be approximated
using a Poisson distribution. This information aids in optimizing
service capacity and wait time estimation.</p></li>
<li><p>Molecular Events in Genetics: The occurrence of genetic mutations
or recombination events in a DNA sequence can be modeled using a Poisson
distribution. This helps researchers study genetic variations and
evolutionary processes.</p></li>
</ul>
</div>
</div>
<div id="random-variable-a-continuous-type" class="section level2">
<h2>5. Random variable: a Continuous Type</h2>
<p>We spoke about RVs that can take values from universal set <span
class="math inline">\(\Omega\)</span> which is either finit or countably
infinite; these were discrete-type random variables. But what if <span
class="math inline">\(\Omega\)</span> is uncountably infinite,i.e. what
if random variable <span class="math inline">\(X\)</span> can take any
real number as a value? Then we have <em>(absolutely) continuous</em>
random variable.</p>
<p>As with discrete-type RVs, continuous-type RVs have their
distribution. With discrete RVs we use probability mass function (p.m.f)
to describe their distribution. With continuous RVs, we have
<em>probability density function (p.d.f)</em> which ‘describes’ them.
One p.d.f. <span class="math inline">\(\varphi(x)\)</span> of a
continuous random variable may look like this:</p>
<p><img src="_img/ContinuousProbability.jpeg" /> Each p.d.f of a
continuous RV should be defined, continuous and non-negative almost
everywhere on the whole real-number line, and:</p>
<p><span class="math display">\[\int_{-\infty}^{+\infty}\varphi(x)dx =
1.\]</span></p>
<p>Don’t worry about the integral - we are not going to compute
integrals in this course. But we do need them to calculate probabilities
for continuous RVs. The integral above tells us that the area under the
curve of p.d.f. is always equal to 1. This seems familiar? This is
actually completely analogous to the fact that, for discrete RV, all
probabilities in its p.m.f. need always to sum to 1.</p>
<p>So, what’s the probability of RV, having p.d.f as in the figure
above, to take value -5.5? It’s</p>
<p><span class="math display">\[P(X = -0.5) = 0.\]</span></p>
<p>OK, that kinda makes sense. But, what about the probability of taking
value 0?</p>
<p><span class="math display">\[P(X = 0) = 0.\]</span></p>
<p>Wait, what? And taking value <span
class="math inline">\(\sqrt{2}\)</span>?</p>
<p><span class="math display">\[P(X = \sqrt{2}) = 0.\]</span></p>
<p>Is it always going to be zero? Yes. Because the probability of
hitting a point exactly you want, out of uncountably infinitely many
others is - of course, zero. But how we can calculate probabilities at
all, if the probability of every point is zero? We just need to broaden
our views - not to ask about the probability of hitting a single point
<span class="math inline">\(x\)</span> among ridiculously infinitely
many, but to ask about the probability of hitting any point in the small
interval <span class="math inline">\(\Delta x\)</span> which contains
<span class="math inline">\(x\)</span>… and uncountably infinite many
other points. When working with continuous RVs we can hope to get
positive probability only if we work with intervals, no matter how big
or small.</p>
<p>However, to get positive probabilities, those intervals need to be
either fully or partially on the <em>support</em> of p.d.f. Support is
the part of p.d.f. curve where this function is positive, i.e. <span
class="math inline">\(\varphi(x) &gt; 0\)</span>. For really small
intervals <span class="math inline">\(\Delta x\)</span> that are outside
of the support, i.e. for which <span class="math inline">\(\varphi(x) =
0\)</span>, there is 0 probability for RV <span
class="math inline">\(X\)</span> to take any value from that interval.
On the other hand, if we observe a really small interval <span
class="math inline">\(\Delta x\)</span> around the point where p.d.f.
takes its maximal value - there is the highest probability of RV <span
class="math inline">\(X\)</span> taking values from this interval.</p>
<p>In other words: higher the value of <span
class="math inline">\(\varphi(x)\)</span> for some <span
class="math inline">\(x\)</span> - higher the probability of RV <span
class="math inline">\(X\)</span> getting some value from a very small
interval <span class="math inline">\(\Delta x\)</span> around <span
class="math inline">\(x\)</span>. (but not in <span
class="math inline">\(x\)</span> itself; there, the probability is
zero)</p>
<hr />
<p>So, we can calculate probability of continuous RV in any interval.
How do we calculate probability of RV <span
class="math inline">\(X\)</span> taking values in some interval <span
class="math inline">\([a, b]\)</span> where <span
class="math inline">\(a\)</span> and <span
class="math inline">\(b\)</span> are two numbers? Simply, using the
formula:</p>
<p><span class="math display">\[P(a \leqslant X \leqslant b) =
\int_a^b\varphi(x)dx.\]</span></p>
<p>It’s that integral again… And again it makes sense again. Remember
that, using Kolmogorov Axioms, we defined probability as a measure? We
also mentioned that integral is the measure of the area under the curve.
So, using an integral to measure area under the curve of p.d.f on some
interval <span class="math inline">\([a,b]\)</span>, we are actually
measuring the probability of RV <span class="math inline">\(X\)</span>
to take any value from that interval.</p>
<p>Observing probabilites as integrals, i.e. areas of surfaces under the
p.d.f curve is completely in-sync with Kolmogorov Axioms for
probability-as-a-measure:</p>
<ul>
<li>Both probability and area are measures, and cannot be negative;</li>
<li>Area of an empty set, or a point/line is zero. So is the
probability;</li>
<li>Bigger the area, bigger the probability; and we may encompass big
areas by taking long intervals or intervals around high values of
p.d.f;</li>
<li>We can just sum areas of disjoint figures to get the total area; the
same is for probabilities of disjoint events.</li>
</ul>
<p>One more important point. We saw that for discrete RVs every point
matters (even if it’s infinite). For some discrete RV <span
class="math inline">\(X\)</span> we have</p>
<p><span class="math display">\[P(X \leqslant k) \neq P(X &lt;
k).\]</span></p>
<p>However, this is not the case with continuous RVs; there one point
doesn’t make a difference (why?). So, for some continuous RV <span
class="math inline">\(X\)</span> we have</p>
<p><span class="math display">\[P(a \leqslant X \leqslant b) = P(a
\leqslant X &lt; b) = P(a &lt; X &lt; b).\]</span></p>
<hr />
<p>As discrete RVs have c.d.f, so continuous RVs have one as well.
Cumulative distribution function for a continuous random variable is
given with</p>
<p><span class="math display">\[F(a) = P(X\leqslant a) =
\int_{-\infty}^a\varphi(x)dx.\]</span></p>
<p>C.d.f of a continuous RV <span class="math inline">\(X\)</span> gives
the probability of <span class="math inline">\(X\)</span> taking any
value from the interval <span class="math inline">\((-\infty,
a]\)</span>. Or, area under the p.d.f curve on that interval. Here’s how
one c.d.f. of a continuous RV looks like:</p>
<p><img src="_img/cdf.png" /> C.d.f. is a prety handy tool for computing
probabilities of continuous RVs, even more useful than p.d.f! In fact,
by knowing the c.d.f we can compute probabilites while avoiding
integrals, using this formula:</p>
<p><span class="math display">\[P(a \leqslant X \leqslant b) = F(b) -
F(a).\]</span></p>
<p>And we can also calculate</p>
<p><span class="math display">\[P(X \geqslant a) = 1 - P(X &lt; a) = 1 -
F(a).\]</span></p>
<hr />
<p>For continuous RVs very useful is <em>quantile function</em>, which
is the inverse of a c.d.f:</p>
<p><span class="math display">\[Q(p) = F^{-1}(p),\qquad p\in(0,
1).\]</span></p>
<p>Simply put, for <span class="math inline">\(p = 0.1\)</span> we can
get a value <span class="math inline">\(x\)</span> for which we can
expect to find 10% of points in the interval <span
class="math inline">\((-\infty, x]\)</span> when sampling from a RV
<span class="math inline">\(X\)</span> having c.d.f <span
class="math inline">\(F\)</span>.</p>
<p>The most important values for <span class="math inline">\(p\)</span>
for the quantile functions are 0.25, 0.5 and 0.75, which give values for
quartiles Q1, Q2 (median) and Q3.</p>
<div id="normal-or-gaussian-distribution" class="section level3">
<h3>Normal (or Gaussian) distribution</h3>
<p>Normal Distribution, denoted by <span
class="math inline">\(\mathcal{N}(\mu, \sigma^2)\)</span>, has two
parameters:</p>
<ul>
<li><p><span class="math inline">\(\mu\)</span>: <em>mean</em>, which
dictates the peak of the bell-curve, i.e. a point which neighbourhood
should be most densly populated by samples;</p></li>
<li><p><span class="math inline">\(\sigma\)</span>: <em>standard
deviation</em>, which dictates the thickness of the bell-curve, i.e. the
dispersion of the sample away from the mean.</p></li>
</ul>
<p><span class="math inline">\(\sigma^2\)</span> which appears in the
notation above is called <em>variance</em>.</p>
<p>The p.d.f for RV with Normal Distribution <span
class="math inline">\(X\sim\mathcal{N}(\mu, \sigma^2)\)</span> is given
by</p>
<p><span class="math display">\[\varphi(x) =
\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}\big(\frac{x-\mu}{\sigma}\big)^2}.\]</span></p>
<p>A very special Normal Distribution is the one with <span
class="math inline">\(\mathcal{N}(0, 1)\)</span>, and it’s called
<em>Standard Normal Distribution</em>. Its p.d.f. and c.d.f. are</p>
<p><span class="math display">\[\varphi(z) =
\frac{1}{\sqrt{2\pi}}e^{-\frac{z^2}{2}}\]</span></p>
<p><span class="math display">\[\Phi(z) =
\frac{1}{\sqrt{2\pi}}\int_{-\infty}^ze^{-\frac{x^2}{2}}dx.\]</span></p>
<p>It’s integrals again… shouldn’t c.d.f relieve us of calculating
integrals? Well, yes, but there’s no simpler way to represent c.d.f for
Standard Normal Distribution. Luckily, every statistics and probability
textbook has a table of values of <span
class="math inline">\(\Phi(z)\)</span> for various values of <span
class="math inline">\(z\)</span>.</p>
<p>Looking at one of those tables we obtain, for <span
class="math inline">\(Z\sim\mathcal{N}(0, 1)\)</span>,</p>
<p><span class="math display">\[\Phi(-2.27) = P(Z &lt; -2.27) =
0.0116.\]</span></p>
<div id="example" class="section level4">
<h4>Example</h4>
<p><em>Average time for delivery service to deliver food is 30 minutes,
with standard deviance of 10 minutes. Asumming the time for delivery is
random variable with Normal Distribution, find probability of your order
arriving more than five minutes late than average.</em></p>
<p>We have <span class="math inline">\(X\sim\mathcal{N}(30,
100)\)</span> and calculate</p>
<p><span class="math display">\[P(X &gt; 35) = 1 - P(X\leqslant
35).\]</span></p>
<p>But wait, we don’t have c.d.f. for this Normal Distribution, because
it’s not standard. Worry not, we can <em>standardize</em> a RV <span
class="math inline">\(X\sim\mathcal{N}(\mu, \sigma)\)</span> by applying
the following transformation</p>
<p><span class="math display">\[Z = \frac{X - \mu}{\sigma}.\]</span></p>
<p>RV <span class="math inline">\(Z\)</span> has the Standard Normal
Distribution, and we can finde the value of its c.d.f. form the table.
So, we continue our calculation:</p>
<p><span class="math display">\[P(X &gt; 35) = 1 - P(X\leqslant 35) = 1
- P\Big(\frac{X - 30}{10}\leqslant  \frac{35 - 30}{10}\Big) = 1 -
P(Z\leqslant 0.5) = 1 - \Phi(0.5) = 1 - 0.6915 = 0.3085.\]</span></p>
<p>Now, for statistical experiments with continuous outcomes, such as
those modeled by the Normal Distribution, the things are different than
with discrete R.V.s. Recall that <code>P(X == x)</code> - the
probability of some <em>exact</em>, <em>real</em> value - is always
zero. That is simply the nature of the continuum. What we can obtain
from continuous probability functions is a probability that some value
falls in some precisely defined interval. If we would want to do that
from a Probability Density Function, we would need to <em>integrate</em>
that function across that interval. But that is probably what you do not
want to do, because there is a way simpler approach, illustrated with
<code>pbinom()</code> in the example with the Binomial experiment above.
Assume that we are observing people in a population with an average
height of 174 cm, with the standard deviation of 10 cm. What is the
probability to randomly meet anyone who is less than (or equal) 180 cm
tall?</p>
<pre class="r"><code>pnorm(180, mean = 174, sd = 10)</code></pre>
<pre><code>## [1] 0.7257469</code></pre>
<p>And what is the probability to randomly observe a person between 160
cm and 180 cm?</p>
<pre class="r"><code>pnorm(180, mean = 174, sd = 10) - pnorm(160, mean = 174, sd = 10)</code></pre>
<pre><code>## [1] 0.6449902</code></pre>
<p>That is how you obtain the probability of real-valued, continuous
statistical outcomes. Let’s plot the Probability Density and the
Cumulative Distribution functions for this statistical experiment.
Density first:</p>
<pre class="r"><code>observations &lt;- 1:300
normalProbability &lt;- dnorm(observations, mean = 179, sd = 10)
normalProbability</code></pre>
<pre><code>##   [1] 6.309257e-71 3.722639e-70 2.174607e-69 1.257672e-68 7.201308e-68 4.082370e-67 2.291239e-66
##   [8] 1.273167e-65 7.004182e-65 3.814930e-64 2.057182e-63 1.098287e-62 5.805189e-62 3.037902e-61
##  [15] 1.573940e-60 8.073459e-60 4.100041e-59 2.061454e-58 1.026163e-57 5.057269e-57 2.467589e-56
##  [22] 1.192029e-55 5.701085e-55 2.699513e-54 1.265524e-53 5.873709e-53 2.699054e-52 1.227913e-51
##  [29] 5.530710e-51 2.466330e-50 1.088876e-49 4.759516e-49 2.059701e-48 8.824755e-48 3.743331e-47
##  [36] 1.572066e-46 6.536427e-46 2.690711e-45 1.096607e-44 4.424780e-44 1.767622e-43 6.991082e-43
##  [43] 2.737514e-42 1.061269e-41 4.073348e-41 1.547870e-40 5.823376e-40 2.169062e-39 7.998828e-39
##  [50] 2.920369e-38 1.055616e-37 3.777736e-37 1.338487e-36 4.695195e-36 1.630611e-35 5.606657e-35
##  [57] 1.908599e-34 6.432540e-34 2.146384e-33 7.090703e-33 2.319147e-32 7.509729e-32 2.407561e-31
##  [64] 7.641655e-31 2.401345e-30 7.471002e-30 2.301231e-29 7.017760e-29 2.118819e-28 6.333538e-28
##  [71] 1.874372e-27 5.491898e-27 1.593111e-26 4.575376e-26 1.300962e-25 3.662345e-25 1.020731e-24
##  [78] 2.816567e-24 7.694599e-24 2.081177e-23 5.573000e-23 1.477495e-22 3.878112e-22 1.007794e-21
##  [85] 2.592865e-21 6.604580e-21 1.665588e-20 4.158599e-20 1.027977e-19 2.515806e-19 6.095758e-19
##  [92] 1.462296e-18 3.472963e-18 8.166236e-18 1.901082e-17 4.381639e-17 9.998379e-17 2.258809e-16
##  [99] 5.052271e-16 1.118796e-15 2.452855e-15 5.324148e-15 1.144156e-14 2.434321e-14 5.127754e-14
## [106] 1.069384e-13 2.207990e-13 4.513544e-13 9.134720e-13 1.830332e-12 3.630962e-12 7.131328e-12
## [113] 1.386680e-11 2.669557e-11 5.088140e-11 9.601433e-11 1.793784e-10 3.317884e-10 6.075883e-10
## [120] 1.101576e-09 1.977320e-09 3.513955e-09 6.182621e-09 1.076976e-08 1.857362e-08 3.171349e-08
## [127] 5.361035e-08 8.972435e-08 1.486720e-07 2.438961e-07 3.961299e-07 6.369825e-07 1.014085e-06
## [134] 1.598374e-06 2.494247e-06 3.853520e-06 5.894307e-06 8.926166e-06 1.338302e-05 1.986555e-05
## [141] 2.919469e-05 4.247803e-05 6.119019e-05 8.726827e-05 1.232219e-04 1.722569e-04 2.384088e-04
## [148] 3.266819e-04 4.431848e-04 5.952532e-04 7.915452e-04 1.042093e-03 1.358297e-03 1.752830e-03
## [155] 2.239453e-03 2.832704e-03 3.547459e-03 4.398360e-03 5.399097e-03 6.561581e-03 7.895016e-03
## [162] 9.404908e-03 1.109208e-02 1.295176e-02 1.497275e-02 1.713686e-02 1.941861e-02 2.178522e-02
## [169] 2.419707e-02 2.660852e-02 2.896916e-02 3.122539e-02 3.332246e-02 3.520653e-02 3.682701e-02
## [176] 3.813878e-02 3.910427e-02 3.969525e-02 3.989423e-02 3.969525e-02 3.910427e-02 3.813878e-02
## [183] 3.682701e-02 3.520653e-02 3.332246e-02 3.122539e-02 2.896916e-02 2.660852e-02 2.419707e-02
## [190] 2.178522e-02 1.941861e-02 1.713686e-02 1.497275e-02 1.295176e-02 1.109208e-02 9.404908e-03
## [197] 7.895016e-03 6.561581e-03 5.399097e-03 4.398360e-03 3.547459e-03 2.832704e-03 2.239453e-03
## [204] 1.752830e-03 1.358297e-03 1.042093e-03 7.915452e-04 5.952532e-04 4.431848e-04 3.266819e-04
## [211] 2.384088e-04 1.722569e-04 1.232219e-04 8.726827e-05 6.119019e-05 4.247803e-05 2.919469e-05
## [218] 1.986555e-05 1.338302e-05 8.926166e-06 5.894307e-06 3.853520e-06 2.494247e-06 1.598374e-06
## [225] 1.014085e-06 6.369825e-07 3.961299e-07 2.438961e-07 1.486720e-07 8.972435e-08 5.361035e-08
## [232] 3.171349e-08 1.857362e-08 1.076976e-08 6.182621e-09 3.513955e-09 1.977320e-09 1.101576e-09
## [239] 6.075883e-10 3.317884e-10 1.793784e-10 9.601433e-11 5.088140e-11 2.669557e-11 1.386680e-11
## [246] 7.131328e-12 3.630962e-12 1.830332e-12 9.134720e-13 4.513544e-13 2.207990e-13 1.069384e-13
## [253] 5.127754e-14 2.434321e-14 1.144156e-14 5.324148e-15 2.452855e-15 1.118796e-15 5.052271e-16
## [260] 2.258809e-16 9.998379e-17 4.381639e-17 1.901082e-17 8.166236e-18 3.472963e-18 1.462296e-18
## [267] 6.095758e-19 2.515806e-19 1.027977e-19 4.158599e-20 1.665588e-20 6.604580e-21 2.592865e-21
## [274] 1.007794e-21 3.878112e-22 1.477495e-22 5.573000e-23 2.081177e-23 7.694599e-24 2.816567e-24
## [281] 1.020731e-24 3.662345e-25 1.300962e-25 4.575376e-26 1.593111e-26 5.491898e-27 1.874372e-27
## [288] 6.333538e-28 2.118819e-28 7.017760e-29 2.301231e-29 7.471002e-30 2.401345e-30 7.641655e-31
## [295] 2.407561e-31 7.509729e-32 2.319147e-32 7.090703e-33 2.146384e-33 6.432540e-34</code></pre>
<pre class="r"><code>normalProbability &lt;- data.frame(observations = observations,
                                density = normalProbability)
ggplot(normalProbability, 
       aes(x = observations, 
           y = density)) + 
  geom_bar(stat = &quot;identity&quot;, 
           fill = &#39;darkred&#39;,
           color = &#39;darkred&#39;) +
  ggtitle(&quot;Gaussian Distribution&quot;) +
  theme_minimal() + 
  theme(panel.border = element_blank()) + 
  theme(plot.title = element_text(hjust = .5, size = 10))</code></pre>
<p><img src="s06_Probability_files/figure-html/unnamed-chunk-51-1.png" width="672" /></p>
<pre class="r"><code>observations &lt;- 1:300
normalProbability &lt;- pnorm(observations, mean = 174, sd = 10)
sum(normalProbability)</code></pre>
<pre><code>## [1] 126.5</code></pre>
<pre class="r"><code>normalProbability &lt;- data.frame(observations = observations,
                                density = normalProbability)
ggplot(normalProbability, 
       aes(x = observations, 
           y = density)) + 
  geom_bar(stat = &quot;identity&quot;, 
           fill = &#39;darkred&#39;,
           color = &#39;darkred&#39;) +
  ggtitle(&quot;Gaussian Cumulative Distribution&quot;) +
  ylab(&quot;P(height &lt;= x)&quot;) + 
  theme_bw() + 
  theme(panel.border = element_blank()) +
  theme(plot.title = element_text(hjust = .5, size = 10))</code></pre>
<p><img src="s06_Probability_files/figure-html/unnamed-chunk-53-1.png" width="672" /></p>
<p>Here are some examples of the normal distribution in nature and
society:</p>
<ul>
<li><p>Heights of Individuals: The distribution of heights in a
population often follows a normal distribution. The majority of
individuals tend to cluster around the mean height, with fewer
individuals at both extremes (very tall or very short).</p></li>
<li><p>IQ Scores: Intelligence quotient (IQ) scores are often
approximated by a normal distribution. In a large population, most
individuals fall around the average IQ score, with fewer individuals
having scores that deviate significantly from the mean.</p></li>
<li><p>Body Mass Index (BMI): The distribution of BMI scores, which
relates to body weight and height, tends to approximate a normal
distribution. In a given population, most individuals have BMI values
near the mean, with fewer individuals having extremely high or low BMI
values.</p></li>
<li><p>Test Scores: In educational settings, the distribution of test
scores often follows a normal distribution. When a large number of
students take a test, their scores tend to form a bell curve, with most
scores clustering around the average and fewer scores at the
extremes.</p></li>
<li><p>Errors in Measurements: Errors in measurement, such as in
scientific experiments or quality control processes, often exhibit a
normal distribution. This phenomenon is known as measurement error or
random error, where the errors tend to center around zero with
decreasing frequency as the magnitude of the error increases.</p></li>
<li><p>Natural Phenomena: Many natural phenomena, such as the
distribution of particle velocities in a gas, the sizes of raindrops, or
the distribution of reaction times in human perception, can be modeled
by the normal distribution.</p></li>
<li><p>Error Terms in Regression Analysis: In regression analysis, the
assumption of normally distributed error terms is often made. This
assumption allows for efficient statistical inference and hypothesis
testing.</p></li>
</ul>
</div>
</div>
</div>
<div id="the-sampling-distribution-of-the-sample-mean"
class="section level2">
<h2>6 The sampling distribution of the sample mean</h2>
<p>Let’s assume that some quantity in reality follows a Normal
Distribution with <span class="math inline">\(\mu\)</span> = 100 and
<span class="math inline">\(\sigma\)</span> = 12, and draw many, many
random samples of size <code>100</code> from this distribution:</p>
<pre class="r"><code>n_samples &lt;- 100000
sampleMeans &lt;- sapply(1:n_samples,
                      function(x) {
                        mean(rnorm(n = 100, mean = 100, sd = 12))
                        })
sampleMeans &lt;- data.frame(sampleMean = sampleMeans)
ggplot(sampleMeans, 
       aes(x = sampleMean)) + 
  geom_histogram(binwidth = .1, 
                 fill = &#39;white&#39;, 
                 color = &#39;purple&#39;) +
  ggtitle(&quot;Sampling Distribution of the Sample Mean&quot;) +
  xlab(&quot;Sample Mean&quot;) + 
  theme_bw() + 
  theme(panel.border = element_blank()) +
  theme(plot.title = element_text(hjust = .5, size = 10))</code></pre>
<p><img src="s06_Probability_files/figure-html/unnamed-chunk-54-1.png" width="672" /></p>
<p>What is the standard deviation of <code>sampleMeans</code>?</p>
<pre class="r"><code>sd(sampleMeans$sampleMean)</code></pre>
<pre><code>## [1] 1.201998</code></pre>
<p>And recall that the <strong>population standard deviation</strong>
was defined to be <strong>12</strong>. Now:</p>
<pre class="r"><code>12/sqrt(100)</code></pre>
<pre><code>## [1] 1.2</code></pre>
<p>And this is roughly the same, right?</p>
<p>The standard deviation of the <strong>sampling distribution of a
mean</strong> - also known as the <strong>standard error</strong> - is
equal to the standard deviation of the <strong>population</strong>
(i.e. the <em>true distribution</em>) divided by <span
class="math inline">\(\sqrt(N)\)</span>:</p>
<p><span class="math display">\[\sigma_\widetilde{x} =
\sigma/\sqrt(N)\]</span></p>
<p>and that means that if we know the <strong>sample standard
deviation</strong></p>
<blockquote>
<p>Standard deviation (SD) measures the dispersion of a dataset relative
to its mean. Standard error of the mean (SEM) measured how much
discrepancy there is likely to be in a sample’s mean compared to the
population mean. The SEM takes the SD and divides it by the square root
of the sample size. Source: <a
href="https://www.investopedia.com/ask/answers/042415/what-difference-between-standard-error-means-and-standard-deviation.asp">Investopedia,
Standard Error of the Mean vs. Standard Deviation: The
Difference</a></p>
</blockquote>
</div>
<div id="the-central-limit-theorem" class="section level2">
<h2>7 The Central Limit Theorem</h2>
<p>In the following set of numerical simulations we want to do the
following:</p>
<ul>
<li>define a probability distribution by providing a set of parameters,
say <span class="math inline">\(\mu\)</span> and <span
class="math inline">\(\sigma^2\)</span> for the Normal, or <span
class="math inline">\(\lambda\)</span> for the Poisson, or
<strong>n</strong> and <strong>p</strong> for the Binomial;</li>
<li>each time we pick a distribution, we take</li>
<li>a random sample of size <code>sampleN &lt;- 100</code>,</li>
<li>compute the <strong>mean</strong> of the obtained random numbers,
and then</li>
<li>we repeat that either 10, or 100, or 1,000, or 10,000 times (varies
as: <code>meanSizes &lt;- c(10, 100, 1000, 10000)</code>).</li>
</ul>
<p>Let’s see what happens:</p>
<pre class="r"><code>sampleN &lt;- 100
meanSizes &lt;- c(10, 100, 1000, 10000)</code></pre>
<p>Poisson with <span class="math inline">\(\lambda = 1\)</span>:</p>
<pre class="r"><code># - Poisson with lambda = 1
lambda = 1
# - Set plot  parameters
par(mfrow = c(2, 2))
# - Plot!
for (meanSize in meanSizes) {
  poisMeans &lt;- sapply(1:meanSize, 
                            function(x) {
                              mean(rpois(sampleN, lambda))
                              }
                     )
  hist(poisMeans, 50, col=&quot;red&quot;,
       main = paste0(&quot;N samples = &quot;, meanSize),
       cex.main = .75)
}</code></pre>
<p><img src="s06_Probability_files/figure-html/unnamed-chunk-58-1.png" width="672" /></p>
<p>Poisson with <span class="math inline">\(\lambda = 2\)</span>:</p>
<pre class="r"><code># - Poisson with lambda = 2
lambda = 2
# - Set plot  parameters
par(mfrow = c(2, 2))
# - Plot!
for (meanSize in meanSizes) {
  poisMeans &lt;- sapply(1:meanSize, 
                            function(x) {
                              mean(rpois(sampleN, lambda))
                              }
                     )
  hist(poisMeans, 50, col=&quot;red&quot;,
       main = paste0(&quot;N samples = &quot;, meanSize),
       cex.main = .75)
}</code></pre>
<p><img src="s06_Probability_files/figure-html/unnamed-chunk-59-1.png" width="672" /></p>
<p>Poisson with <span class="math inline">\(\lambda = 10\)</span>:</p>
<pre class="r"><code># - Poisson with lambda = 10
lambda = 10
# - Set plot  parameters
par(mfrow = c(2, 2))
# - Plot!
for (meanSize in meanSizes) {
  poisMeans &lt;- sapply(1:meanSize, 
                            function(x) {
                              mean(rpois(sampleN, lambda))
                              }
                     )
  hist(poisMeans, 50, col=&quot;red&quot;,
       main = paste0(&quot;N samples = &quot;, meanSize),
       cex.main = .75)
}</code></pre>
<p><img src="s06_Probability_files/figure-html/unnamed-chunk-60-1.png" width="672" />
Binomial with <strong>p</strong> = .1 and <strong>n</strong> = 1000:</p>
<pre class="r"><code># - Binomial with p = .1 and n = 1000
p = .1
n = 1000
# - Set plot  parameters
par(mfrow = c(2, 2))
# - Plot!
for (meanSize in meanSizes) {
  binomMeans &lt;- sapply(1:meanSize, 
                            function(x) {
                              mean(rbinom(n = sampleN, size = n, prob = p))                              }
                     )
  hist(binomMeans, 50, col=&quot;darkorange&quot;,
       main = paste0(&quot;N samples = &quot;, meanSize),
       cex.main = .75)
}</code></pre>
<p><img src="s06_Probability_files/figure-html/unnamed-chunk-61-1.png" width="672" /></p>
<p>Binomial with <strong>p</strong> = .5 and <strong>n</strong> =
1000:</p>
<pre class="r"><code># - Binomial with p = .5 and n = 1000
p = .5
n = 100
# - Set plot  parameters
par(mfrow = c(2, 2))
# - Plot!
for (meanSize in meanSizes) {
  binomMeans &lt;- sapply(1:meanSize, 
                            function(x) {
                              mean(rbinom(n = sampleN, size = n, prob = p))
                              }
                     )
  hist(binomMeans, 50, col=&quot;darkorange&quot;,
       main = paste0(&quot;N samples = &quot;, meanSize),
       cex.main = .75)
}</code></pre>
<p><img src="s06_Probability_files/figure-html/unnamed-chunk-62-1.png" width="672" />
Normal with <span class="math inline">\(\mu = 10\)</span> and <span
class="math inline">\(\sigma = 1.5\)</span>:</p>
<pre class="r"><code># - Normal with mean = 10 and sd = 1.5
mean = 10
sd = 1.5
# - Set plot  parameters
par(mfrow = c(2, 2))
# - Plot!
for (meanSize in meanSizes) {
  normalMeans &lt;- sapply(1:meanSize, 
                            function(x) {
                              mean(rnorm(sampleN, mean = mean, sd = sd))
                              }
                     )
  hist(normalMeans, 50, col=&quot;lightblue&quot;,
       main = paste0(&quot;N samples = &quot;, meanSize),
       cex.main = .75)
}</code></pre>
<p><img src="s06_Probability_files/figure-html/unnamed-chunk-63-1.png" width="672" />
Normal with <span class="math inline">\(\mu = 175\)</span> and <span
class="math inline">\(\sigma = 11.4\)</span>:</p>
<pre class="r"><code># - Normal with mean = 175 and sd = 11.4
mean = 175
sd = 11.4
# - Set plot  parameters
par(mfrow = c(2, 2))
# - Plot!
for (meanSize in meanSizes) {
  normalMeans &lt;- sapply(1:meanSize, 
                            function(x) {
                              mean(rnorm(sampleN, mean = mean, sd = sd))
                              }
                     )
  hist(normalMeans, 50, col=&quot;lightblue&quot;,
       main = paste0(&quot;N samples = &quot;, meanSize),
       cex.main = .75)
}</code></pre>
<p><img src="s06_Probability_files/figure-html/unnamed-chunk-64-1.png" width="672" />
Normal with <span class="math inline">\(\mu = 100\)</span> and <span
class="math inline">\(\sigma = 25\)</span>:</p>
<pre class="r"><code># - Normal with mean = 100 and sd = 25
mean = 100
sd = 25
# - Set plot  parameters
par(mfrow = c(2, 2))
# - Plot!
for (meanSize in meanSizes) {
  normalMeans &lt;- sapply(1:meanSize, 
                            function(x) {
                              mean(rnorm(sampleN, mean = mean, sd = sd))
                              }
                     )
  hist(normalMeans, 50, col=&quot;lightblue&quot;,
       main = paste0(&quot;N samples = &quot;, meanSize),
       cex.main = .75)
}</code></pre>
<p><img src="s06_Probability_files/figure-html/unnamed-chunk-65-1.png" width="672" /></p>
<p>What happens if we start increasing <code>sampleN</code> and keep the
number of samples at some decent number, say
<code>meanSize = 10000</code>?</p>
<pre class="r"><code>sampleN &lt;- c(10, 100, 1000, 100000)
meanSize &lt;- c(10000)</code></pre>
<p>Poisson with <span class="math inline">\(\lambda = 3\)</span>:</p>
<pre class="r"><code># - Poisson with lambda = 1
lambda = 3
# - Set plot  parameters
par(mfrow = c(2, 2))
# - Plot!
for (sampleN in sampleN) {
  poisMeans &lt;- sapply(1:meanSize, 
                            function(x) {
                              mean(rpois(sampleN, lambda))
                              }
                     )
  hist(poisMeans, 50, col=&quot;red&quot;,
       main = paste0(&quot;Sample N = &quot;, sampleN),
       cex.main = .75)
}</code></pre>
<p><img src="s06_Probability_files/figure-html/unnamed-chunk-67-1.png" width="672" /></p>
<p>Normal with <span class="math inline">\(\mu = 10\)</span> and <span
class="math inline">\(\sigma = .75\)</span>:</p>
<pre class="r"><code>sampleN &lt;- c(10, 100, 1000, 100000)
meanSize &lt;- c(10000)</code></pre>
<pre class="r"><code># - Normal with mean = 10 and sd = .75
mean = 10
sd = .75
# - Set plot  parameters
par(mfrow = c(2, 2))
# - Plot!
for (sampleN in sampleN) {
  normMeans &lt;- sapply(1:meanSize, 
                            function(x) {
                              mean(rnorm(sampleN, mean = mean, sd = sd))
                              }
                     )
  hist(normMeans, 50, col=&quot;lightblue&quot;,
       main = paste0(&quot;Sample N = &quot;, sampleN),
       cex.main = .75)
}</code></pre>
<p><img src="s06_Probability_files/figure-html/unnamed-chunk-69-1.png" width="672" /></p>
<p>All the same as the number of samples increase?
<strong>Exactly</strong>:</p>
<blockquote>
<p>Central Limit Theorem. In probability theory, the central limit
theorem (CLT) establishes that, in many situations, when independent
random variables are added, their properly normalized sum tends toward a
normal distribution (informally a bell curve) even if the original
variables themselves are not normally distributed. The theorem is a key
concept in probability theory because it implies that probabilistic and
statistical methods that work for normal distributions can be applicable
to many problems involving other types of distributions. Source: <a
href="https://en.wikipedia.org/wiki/Central_limit_theorem">Central Limit
Theorem, English Wikipedia, accessed: 2021/02/17</a></p>
</blockquote>
<p>Does it <strong>always work</strong>? Strictly: NO. Enter <a
href="https://en.wikipedia.org/wiki/Cauchy_distribution">Cauchy
Distribution</a>:</p>
<pre class="r"><code># - Cauchy with location = 0, scale = 1
# - set plot  parameters
par(mfrow=c(2,2))
# - plot!
for (meanSize in c(100, 1000, 10000, 100000)) {
  cauchySums &lt;- unlist(lapply(seq(1:meanSize), function(x) {
    sum(rcauchy(1000, location = 0, scale = 1))
  }))
  hist(cauchySums, 50, col=&quot;orange&quot;,
       main = paste(&quot;N samples = &quot;, meanSize, sep=&quot;&quot;),
       cex.main = .75)
}</code></pre>
<p><img src="s06_Probability_files/figure-html/unnamed-chunk-70-1.png" width="672" /></p>
<p>… also known as <a
href="https://en.wikipedia.org/wiki/Witch_of_Agnesi">The Witch of
Agnesi</a>…</p>
<hr />
<div id="further-readings" class="section level3">
<h3>Further Readings</h3>
<ul>
<li><a
href="https://towardsdatascience.com/probability-concepts-explained-probability-distributions-introduction-part-3-4a5db81858dc">Probability
concepts explained: probability distributions (introduction part 3) by
Jonny Brooks-Bartlett, from Towards Data Science</a></li>
<li><a href="https://math.dartmouth.edu/~prob/prob/prob.pdf">Grinstead
and Snell’s Introduction to Probability (NOTE: The Bible of Probability
Theory)</a>. Definitely not an introductory material, but everything
from Chapter 1. and up to Chapter 9. at least is (a) super-interesting
to learn, (b) super-useful in Data Science, and (c) most Data Science
practitioners already know it (or should know it). Enjoy!</li>
<li><a
href="https://www.itl.nist.gov/div898/handbook/eda/section3/eda35g.htm">More
on the Kolmogorov-Smirnov Test</a></li>
<li><a href="https://ggplot2.tidyverse.org/reference/geom_qq.html">More
on QQ plots w. {ggplot2}</a></li>
<li><a
href="https://www.econometrics-with-r.org/2-1-random-variables-and-probability-distributions.html">More
on Probability Functions in R: 2.1 Random Variables and Probability
Distributions from Introduction to Econometrics with R</a></li>
</ul>
</div>
<div id="some-introductory-video-material" class="section level3">
<h3>Some Introductory Video Material</h3>
<ul>
<li><a
href="https://www.khanacademy.org/math/algebra/x2f8bb11595b61c86:functions">Khan
Academy: Functions</a></li>
<li><a
href="https://www.khanacademy.org/math/statistics-probability/probability-library/basic-theoretical-probability/v/basic-probability">Khan
Academy: Basic theoretical probability</a></li>
<li><a
href="https://www.khanacademy.org/math/statistics-probability/probability-library/probability-sample-spaces/v/events-and-outcomes-3">Khan
Academy: Probability Using Sample Spaces</a></li>
<li><a
href="https://www.khanacademy.org/math/statistics-probability/random-variables-stats-library/random-variables-discrete/v/random-variables">Khan
Academy: Discrete Random Variables</a></li>
<li><a
href="https://www.khanacademy.org/math/statistics-probability/random-variables-stats-library/random-variables-continuous/v/probability-density-functions">Khan
Academy: Continuous Random Variables</a></li>
</ul>
</div>
<div id="r-markdown" class="section level3">
<h3>R Markdown</h3>
<p><a href="https://rmarkdown.rstudio.com/">R Markdown</a> is what I
have used to produce this beautiful Notebook. We will learn more about
it near the end of the course, but if you already feel ready to dive
deep, here’s a book: <a href="https://bookdown.org/yihui/rmarkdown/">R
Markdown: The Definitive Guide, Yihui Xie, J. J. Allaire, Garrett
Grolemunds.</a></p>
<hr />
<p>License: <a href="http://www.gnu.org/licenses/gpl-3.0.txt">GPLv3</a>
This Notebook is free software: you can redistribute it and/or modify it
under the terms of the GNU General Public License as published by the
Free Software Foundation, either version 3 of the License, or (at your
option) any later version. This Notebook is distributed in the hope that
it will be useful, but WITHOUT ANY WARRANTY; without even the implied
warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
GNU General Public License for more details. You should have received a
copy of the GNU General Public License along with this Notebook. If not,
see <a href="http://www.gnu.org/licenses/"
class="uri">http://www.gnu.org/licenses/</a>.</p>
<hr />
</div>
</div>
</div>

<hr>
<p style="text-align:center;">
  <img src="DK_Logo_White_150.png">
</p>
<p style="font-size:13px;text-align:center;">
  <b>Contact: </b><a href = "mailto: goran.milovanovic@datakolektiv.com">goran.milovanovic@datakolektiv.com</a><br><br><a href="https://github.com/datakolektiv"><img src="github.png"></a>&nbsp;&nbsp;<a href="https://www.linkedin.com/in/gmilovanovic/"><img src="linkedin.png"></a>
</p>
<p style="font-size:11px;text-align:center;"><b>Impressum</b><br>
<b>Data Kolektiv, 2004, Belgrade.</b></p>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3,h4,h5",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
