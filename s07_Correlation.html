<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Session07 Covariance, Correlation, Simple Linear Regression</title>

<script src="site_libs/header-attrs-2.29/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.13.2/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html"><img id="logo" style="width: 30px;" src="DK_Logo_White_NoTitle_25.png" /></a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Sessions
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="s00_installations.html">S00. R and Python Installations</a>
    </li>
    <li>
      <a href="s01_introduction_to_r.html">S01. Introduction to R</a>
    </li>
    <li>
      <a href="s02_dataframe.html">S02. I/O, Packages, and data.frame</a>
    </li>
    <li>
      <a href="s03_functional.html">S03. Functional programming + Control flow in R</a>
    </li>
    <li>
      <a href="s04_more_functional.html">S04. More functional Programming + Basic Statistics in R</a>
    </li>
    <li>
      <a href="s05_EDA.html">S05. Exploratory Data Analysis (EDA)</a>
    </li>
    <li>
      <a href="s06_Probability.html">S06. Probability Theory</a>
    </li>
    <li>
      <a href="s07_Correlation.html">S07. Covariance, Correlation, Regression</a>
    </li>
    <li>
      <a href="s08_LeastSquares.html">S08. Estimation Theory - Least Squares</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Labs
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="l01_lists.html">L01. Data Types, Lists</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Notes
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="n00_installations.html">N00. R and Python Installations</a>
    </li>
    <li>
      <a href="n03_functional.html">N03. Functional Programming</a>
    </li>
    <li>
      <a href="n04_more_functional.html">N04. More Functional Programming</a>
    </li>
  </ul>
</li>
<li>
  <a href="video.html">Video</a>
</li>
<li>
  <a href="resources.html">Resources</a>
</li>
<li>
  <a href="about.html">About</a>
</li>
<li>
  <a href="media.html">Media</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Session07 Covariance, Correlation, Simple
Linear Regression</h1>
<h4 class="author">Goran S. Milovanovic, PhD</h4>

</div>


<p><img src="DK_Logo_100.png" /></p>
<hr />
<div id="covariance-correlation-simple-linear-regression"
class="section level1">
<h1>Covariance, Correlation, Simple Linear Regression</h1>
<hr />
<div id="what-do-we-want-to-do-today" class="section level3">
<h3>What do we want to do today?</h3>
<p>Today, we begin laying the fundamental mathematical foundations for
working in Data Science, which includes an introduction to probability
theory. Probability theory is a branch of mathematics that deals with
uncertain events—events that, as the name suggests, can occur only with
some probability. But what does that mean? If something is uncertain,
inherently unpredictable, is it even possible to describe it
mathematically and treat it with the appropriate mathematical framework?
The development of mathematics over the past three centuries,
culminating in the 20th century, has shown that this is indeed possible.
Probability theory is a mathematical discipline that connects the
natural language we use to intuitively express ourselves about uncertain
events with precise mathematical expressions. Through set theory,
algebra, combinatorics, and mathematical analysis, it allows us to
incorporate uncertain events into science and engineering.</p>
<p>The fundamental concepts we are introducing in this session are:
event algebra, probability, random variables, distributions (discrete
and continuous) of random variables, and the central limit theorem,
whose significance in many empirical domains cannot be overstated.</p>
</div>
<div id="what-do-we-want-to-do-today-1" class="section level3">
<h3>What do we want to do today?</h3>
<p>Correlations, correlations everywhere..! One would think that all
that Data Scientists do nowadays is to look for them. Not even half
true, however, they are of essential importance for our work. In this
session, we introduce the concept of correlation, and expand it - in a
gentle way - towards simple linear regression.</p>
</div>
<div id="prerequisits" class="section level3">
<h3>0. Prerequisits</h3>
<p>Install:</p>
<pre class="r"><code># install.packages(&#39;corrplot&#39;)
# install.packages(&#39;Hmisc&#39;)
# install.packages(&#39;ppcor&#39;)
dataDir &lt;- paste0(getwd(), &quot;/_data/&quot;)</code></pre>
<p>Setup:</p>
<pre class="r"><code>library(tidyverse)
library(Hmisc)
library(ppcor)
library(corrplot)</code></pre>
</div>
<div id="covariance-variable-standardization-and-correlation"
class="section level3">
<h3>1. Covariance, Variable Standardization, and Correlation</h3>
<div id="linear-relationships" class="section level4">
<h4>1.1 Linear relationships</h4>
<p>We will start by inspecting two variables from the iris data set:
<code>Sepal.Length</code> and <code>Petal.Length</code>:</p>
<pre class="r"><code># - plot layout: 2 x 2
par(mfcol = c(2, 2))
# - boxplot iris$Sepal.Length
boxplot(iris$Sepal.Length,
        horizontal = TRUE, 
        xlab = &quot;Sepal Length&quot;)
# - histogram: iris$Sepal.Length
hist(iris$Sepal.Length, 
     main = &quot;&quot;,
     xlab = &quot;Sepal.Length&quot;, 
     prob = T)
# - overlay iris$Sepal.Length density 
# - function over the empirical distribution
lines(density(iris$Sepal.Length),
      lty = &quot;dashed&quot;, 
      lwd = 2.5, 
      col = &quot;red&quot;)
# - boxplot iris$Petal.Length
boxplot(iris$Petal.Length,
        horizontal = TRUE, 
        xlab = &quot;Petal Length&quot;)
# - histogram: iris$Petal.Length,
hist(iris$Petal.Length,
     main = &quot;&quot;,
     xlab = &quot;Petal Length&quot;, 
     prob = T)
# - overlay iris$Petal.Length density 
# - function over the empirical distribution
lines(density(iris$Petal.Length),
      lty = &quot;dashed&quot;, 
      lwd = 2.5, 
      col = &quot;red&quot;)</code></pre>
<p><img src="s07_Correlation_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<pre class="r"><code># reset plot layout
par(mfcol = c(1, 1))</code></pre>
<p><strong>Q.</strong> Is there a linear relationship between these two
variables? Let’s see:</p>
<pre class="r"><code># scatter plot w. {base}
plot(iris$Sepal.Length, iris$Petal.Length,
     main = &quot;Sepal Length vs Petal Length&quot;,
     xlab = &quot;Sepal Length&quot;, ylab = &quot;Petal Length&quot;,
     cex.main = .85,
     cex.lab = .75)</code></pre>
<p><img src="s07_Correlation_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>One could think there is something going on here. But:</p>
<pre class="r"><code>ggplot(data = iris, aes(x = Sepal.Length,
                        y = Petal.Length,
                        color = Species)
       ) + 
  geom_point() + 
  geom_smooth(method = &quot;lm&quot;, se = FALSE, linewidth = .25) +
  theme_bw() + 
  theme(panel.border = element_blank())</code></pre>
<p><img src="s07_Correlation_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>There seems to be more than one important line to describe this data
set… However, we will simplify for now:</p>
<pre class="r"><code>ggplot(data = iris, aes(x = Sepal.Length,
                        y = Petal.Length)) + 
  geom_point(color = &quot;black&quot;, size = 2) + 
  geom_point(color = &quot;white&quot;, size = 1.5) +
  geom_smooth(method = lm, se = F, linewidth = .25, color = &quot;red&quot;) +
  theme_bw() + 
  theme(panel.border = element_blank())</code></pre>
<p><img src="s07_Correlation_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
</div>
<div id="covariance-and-standardization" class="section level4">
<h4>1.2 Covariance and Standardization</h4>
<p>Leaving aside the important question of whether there is a linear
relationship between <code>Sepal.Length</code> and
<code>Petal.Length</code> in <code>iris</code> for now, we ask:
<strong>if it was a linear relationship</strong>, how good a linear
relationship would it make? The answer is provided by computing the
Pearson’s coefficient of linear correlation.</p>
<p>First things first. What is this:</p>
<pre class="r"><code>cov(iris$Sepal.Length, iris$Petal.Length)</code></pre>
<pre><code>## [1] 1.274315</code></pre>
<p><strong>Covariance</strong>. Given two random variables (RVs), <span
class="math inline">\(X\)</span> and <span
class="math inline">\(Y\)</span>, their (sample) covariance is given
by:</p>
<p><span class="math display">\[cov(X,Y) = E[(X-E[X])(Y-E[Y])] =
\frac{(X-\bar{X})(Y-\bar{Y})}{N-1}\]</span> where <span
class="math inline">\(E[]\)</span> denotes the <em>expectation</em> (the
<em>mean</em>, if you prefer), <span
class="math inline">\(\bar{X}\)</span> is the mean of <span
class="math inline">\(X\)</span>, <span
class="math inline">\(\bar{Y}\)</span> is the mean of <span
class="math inline">\(Y\)</span>, and <span
class="math inline">\(N\)</span> is the sample size.</p>
</div>
<div id="correlation" class="section level4">
<h4>1.3 Correlation</h4>
<p>Pearson’s coefficient of correlation is nothing else than a
covariance between <span class="math inline">\(X\)</span> and <span
class="math inline">\(Y\)</span> upon their <em>standardization</em>.
The standardization of a RV - widely known as a variable
<em>z-score</em> - is obtained upon subtracting all of its values from
the mean, and dividing by the standard deviation; for the
<strong>i</strong>-th observation of <span
class="math inline">\(X\)</span>:</p>
<p><span class="math display">\[z(x_i) =
\frac{x_i-\bar{X}}{\sigma}\]</span></p>
<p>Thus,</p>
<pre class="r"><code>zSepalLength &lt;- (iris$Sepal.Length - mean(iris$Sepal.Length))/sd(iris$Sepal.Length)

zPetalLength &lt;- (iris$Petal.Length - mean(iris$Petal.Length))/sd(iris$Petal.Length)

cov(zSepalLength, zPetalLength)</code></pre>
<pre><code>## [1] 0.8717538</code></pre>
<p>is the correlation of <code>Sepal.Length</code> and
<code>Petal.Length</code>; let’s check with {base} R function
<code>cor()</code> which computes the correlation:</p>
<pre class="r"><code>cor(iris$Sepal.Length, iris$Petal.Length, 
    method = &quot;pearson&quot;)</code></pre>
<pre><code>## [1] 0.8717538</code></pre>
<p>Right. There are many formulas that compute <code>r</code>, the
correlation coefficient; however, understanding that is simply the
covariance of standardized RVs is essential. Once you know to
standardize the variables and how to compute covariance (and that is
easy), you don’t need to care about expressions like:</p>
<p><span class="math display">\[r_{XY} =
\frac{N\sum{XY}-(\sum{X})(\sum{Y})}{\sqrt{[N\sum{X^2}-(\sum{X})^2][N\sum{Y^2}-(\sum{Y})^2]}}\]</span></p>
<p>This and similar expressions are good, and especially for two
purposes: first, they will compute the desired value of the correlation
coefficient in the end, that’s for sure, and second, writing them up in
<code>RMarkdown</code> really helps mastering <span
class="math inline">\(\LaTeX\)</span>. Besides these roles they play,
there is really nothing essentially important in relation to them.</p>
<p>Somewhat easier to remember:</p>
<p><span class="math display">\[r_{XY} =
\frac{cov(X,Y)}{\sigma(X)\sigma(Y)}\]</span> - the covariance of <span
class="math inline">\(X\)</span> and <span
class="math inline">\(Y\)</span>, divided by the product of their
standard deviations.</p>
<p>There’s a nice <code>scale()</code> function that will quicken-up the
computation of <em>z-scores</em> in R for you:</p>
<pre class="r"><code>zSepalLength1 &lt;-  scale(iris$Sepal.Length, center = T, scale = T)
sum(zSepalLength1 == zSepalLength) == length(zSepalLength)</code></pre>
<pre><code>## [1] FALSE</code></pre>
<p>Do <code>?scale</code> - useful things can be done with it.</p>
</div>
</div>
<div
id="correlation-matrices-visualization-and-treatment-of-missing-values"
class="section level3">
<h3>2. Correlation Matrices: Visualization and Treatment of Missing
Values</h3>
<p>The {base} <code>cor()</code> function produces correlation matrices
too:</p>
<pre class="r"><code>cor(iris[ , c(1:4)])</code></pre>
<pre><code>##              Sepal.Length Sepal.Width Petal.Length Petal.Width
## Sepal.Length    1.0000000  -0.1175698    0.8717538   0.8179411
## Sepal.Width    -0.1175698   1.0000000   -0.4284401  -0.3661259
## Petal.Length    0.8717538  -0.4284401    1.0000000   0.9628654
## Petal.Width     0.8179411  -0.3661259    0.9628654   1.0000000</code></pre>
<p>Missing data can be treated by <em>listwise</em> or <em>pairwise</em>
deletion. In <em>listwise</em> deletion, any observation (== row)
containing at least one <code>NA</code>(s) will be removed before the
computation. Set the <code>use</code> argument in <code>cor</code> to
<code>complete.obs</code> in order to use listwise deletion:</p>
<pre class="r"><code>dSet &lt;- iris
# Remove one nominal variable - Species
dSet$Species &lt;- NULL
# introduce NA in dSet$Sepal.Length[5]
dSet$Sepal.Length[5] &lt;- NA
# Pairwise and Listwise Deletion:
cor1a &lt;- cor(dSet, 
             use = &quot;complete.obs&quot;) # listwise deletion
cor1a</code></pre>
<pre><code>##              Sepal.Length Sepal.Width Petal.Length Petal.Width
## Sepal.Length    1.0000000  -0.1099640    0.8708659   0.8165243
## Sepal.Width    -0.1099640   1.0000000   -0.4219569  -0.3590627
## Petal.Length    0.8708659  -0.4219569    1.0000000   0.9624228
## Petal.Width     0.8165243  -0.3590627    0.9624228   1.0000000</code></pre>
<p><em>Pairwise deletion</em> will compute the correlation coefficient
using all available data. It will delete only the data corresponding to
the missing values from one vector in another, and compute the
correlation coefficient from what is left; set <code>use</code> to
<code>pairwise.complete.obs</code> to use this approach:</p>
<pre class="r"><code>cor1b &lt;- cor(dSet, use = &quot;pairwise.complete.obs&quot;) # pairwise deletion
cor1b</code></pre>
<pre><code>##              Sepal.Length Sepal.Width Petal.Length Petal.Width
## Sepal.Length    1.0000000  -0.1099640    0.8708659   0.8165243
## Sepal.Width    -0.1099640   1.0000000   -0.4284401  -0.3661259
## Petal.Length    0.8708659  -0.4284401    1.0000000   0.9628654
## Petal.Width     0.8165243  -0.3661259    0.9628654   1.0000000</code></pre>
<p><code>use = "all.obs"</code> will produce an error in the presence of
any <code>NA</code>s:</p>
<pre class="r"><code>cor1c &lt;- cor(dSet, 
             use = &quot;all.obs&quot;) # all observations - error</code></pre>
<pre><code>## Error in cor(dSet, use = &quot;all.obs&quot;): missing observations in cov/cor</code></pre>
<p>To propagate <code>NA</code>s through the matrix wherever they are
present in the respective columns, <code>use = "everything"</code> (this
is the <em>default</em>; try <code>cor(dSet)</code>):</p>
<pre class="r"><code>cor1d &lt;- cor(dSet, use = &quot;everything&quot;)
cor1d</code></pre>
<pre><code>##              Sepal.Length Sepal.Width Petal.Length Petal.Width
## Sepal.Length            1          NA           NA          NA
## Sepal.Width            NA   1.0000000   -0.4284401  -0.3661259
## Petal.Length           NA  -0.4284401    1.0000000   0.9628654
## Petal.Width            NA  -0.3661259    0.9628654   1.0000000</code></pre>
<p>There are many available methods to visualize correlation matrices in
R. The {base} approach would be to use <code>plot()</code> on a
<code>data.frame</code> like in the following example:</p>
<pre class="r"><code># {base} approach
data(&quot;mtcars&quot;)
corMatrix &lt;- cor(mtcars[ , 1:8])
plot(as.data.frame(corMatrix))</code></pre>
<p><img src="s07_Correlation_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>But there’s also the fantastic <code>{corrplot}</code> package to
visualize correlation matrices:</p>
<pre class="r"><code># {corrplot} approach
corMatrix &lt;- cor(mtcars)</code></pre>
<pre class="r"><code># {corrplot} &quot;circle&quot; method: 
corrplot(corMatrix, 
         method = &quot;circle&quot;)</code></pre>
<p><img src="s07_Correlation_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<pre class="r"><code># {corrplot} &quot;ellipse&quot; method: 
corrplot(corMatrix, 
         method = &quot;ellipse&quot;)</code></pre>
<p><img src="s07_Correlation_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<pre class="r"><code># &quot;mixed&quot;
corrplot.mixed(corMatrix, 
               lower = &quot;ellipse&quot;, 
               upper = &quot;circle&quot;)</code></pre>
<p><img src="s07_Correlation_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
</div>
<div id="introduction-to-simple-linear-regression"
class="section level3">
<h3>3. Introduction to Simple Linear Regression</h3>
<p>We now begin considering the mathematical modeling of data in R. The
first - and arguably the simplest - statistical model that we will face
is the <em>Simple Linear Regression Model</em>. In a typical simple
linear regression setting, we have one continuous <em>predictor</em> -
also known as the <em>independent variable</em> - and one continuous
<em>criterion</em> - a.k.a. the <em>dependent variable</em>. Both these
are assumed to be unbounded, i.e. taking values across the whole domain
of real numbers. <em>Continuity</em> here should be understood precisely
as having measurements from an <em>interval</em> or <em>ratio
scale</em>.</p>
<p>Linear regression <em>does not imply any causality</em>; it is up to
the user of the model to impose causal assumptions, i.e. which variable
takes the role of the criterion and which variable is assigned as a
predictor. It is not even necessary to impose any such assumptions in
order to obtain a valid linear regression model, although it is very
customary to have some hypothesized direction of causality in order to
discuss prediction meaningfully.</p>
<div id="linear-correlation-assumption-of-linearity-and-causality"
class="section level4">
<h4>3.1 Linear Correlation, Assumption of Linearity, and Causality</h4>
<pre class="r"><code>## Pearson correlation in R {base}
cor1 &lt;- cor(iris$Sepal.Length, 
            iris$Petal.Length, 
            method = &quot;pearson&quot;)
cor1</code></pre>
<pre><code>## [1] 0.8717538</code></pre>
<p>With <span class="math inline">\(R\)</span> = .87 we hope to be able
to say that there is a linear relationship, right? Time to learn
something important about statistics: you can never rely on a conclusion
that was reached by taking the values of the statistics <em>prima
facie</em> while doing nothing else. Take a look at the scatter plot of
these two variables again:</p>
<pre class="r"><code># Let&#39;s test the assumption of linearity:
ggplot(iris, aes(x = Sepal.Length, y = Petal.Length)) +
  geom_point(size = 2, color = &#39;black&#39;) +
  geom_point(size = 1.5, color = &#39;white&#39;) +
  geom_smooth(method = &#39;lm&#39;, linewidth = .25, color = &#39;red&#39;, se = FALSE) +
  ggtitle(&#39;Sepal Length vs Petal Length&#39;) +
  theme_bw() + 
  theme(panel.border = element_blank())</code></pre>
<p><img src="s07_Correlation_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p>We have included the best fitting regression line in the scatter
plot; does the relationship between the two variable really looks
<em>linear</em>? Let’s remind ourselves of what we have already
discovered:</p>
<pre class="r"><code>ggplot(data = iris, aes(x = Sepal.Length,
                        y = Petal.Length,
                        color = Species)
       ) + 
  geom_point(size = 1.5) +
  geom_smooth(method = &#39;lm&#39;, linewidth = .25, se = F) + 
  ggtitle(&#39;Sepal Length vs Petal Length&#39;) + 
  theme_bw() + 
  theme(panel.border = element_blank())</code></pre>
<p><img src="s07_Correlation_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<p>Interesting: there seem to be <em>more than one</em> linear
relationship in this scatter plot, i.e. one per each group from the
<code>iris</code> data set. What do we do, except for concluding that
the <em>assumption of linearity</em> has failed? We will introduce a fix
in one of our next sessions, showing how a multiple regression model can
account for situations like the present one; in the meantime, pretend
like nothing has happened…</p>
<p>By the way, is the <span class="math inline">\(R\)</span> coefficient
of linear correlation statistically significant?</p>
<pre class="r"><code># Is Pearson&#39;s correlation coefficient significant?
cor2 &lt;- rcorr(iris$Sepal.Length, # {hmisc}
              iris$Petal.Length, 
              type=&quot;pearson&quot;)
# correlations
cor2$r</code></pre>
<pre><code>##           x         y
## x 1.0000000 0.8717538
## y 0.8717538 1.0000000</code></pre>
<pre class="r"><code>cor2$r[1, 2] # Ok, the one we&#39;re looking for</code></pre>
<pre><code>## [1] 0.8717538</code></pre>
<pre class="r"><code>cor2$P[1, 2] # significant at</code></pre>
<pre><code>## [1] 0</code></pre>
<p>Well, <span class="math inline">\(R\)</span> is statistically
significant indeed. Most social science students would typically
conclude that everything’s superfine here… Don’t be lazy: (a) do the EDA
of your variables before modeling, (b) inspect the scatter plot in
<em>smart ways</em> - if there are natural groupings expected in the
data set, use colors or shapes to mark them. In spite of the high,
statistically significant Pearson’s correlation coefficient between
<code>Sepal.Length</code> and <code>Petal.Length</code>, this
relationship violates linearity, and a model more powerful than simple
linear regression is needed.</p>
<p>However, let’s pretend we’ve never seen this and start doing simple
linear regression in R.</p>
</div>
<div id="simple-linear-regression-the-model" class="section level4">
<h4>3.2 Simple Linear Regression: The Model</h4>
<p>We will now consider the following statistical model of a linear
relationship between two random variables:</p>
<p><span class="math display">\[Y = \beta_0 + \beta_1X_1 + \epsilon
\]</span></p>
<ul>
<li><span class="math inline">\(Y\)</span> is the variable whose values
we would like to be able to predict - and it is called a
<em>criterion</em> or a <em>dependent variable</em> - from</li>
<li><span class="math inline">\(X\)</span>, which is called a
<em>predictor</em>, or an <em>independent variable</em> in the Simple
Linear Regression setting;</li>
<li><span class="math inline">\(\beta_0\)</span> and <span
class="math inline">\(\beta_1\)</span> are <em>model parameters</em>, of
which the former represents the <em>intercept</em> while the later is
the <em>slope</em> of the regression line (<strong>note:</strong>
besides <span class="math inline">\(\epsilon\)</span>, what the equation
represents is nothing else but the equation of a straight line in a
plane that you have seen a dozen times in high school); finally,</li>
<li><span class="math inline">\(\epsilon\)</span> represents the model
error term, which we will discuss in length in our Session.</li>
</ul>
<p>If we assume that the relationship between <span
class="math inline">\(X\)</span> and <span
class="math inline">\(Y\)</span> is indeed linear - and introduce some
additional assumptions that we will discuss in our next session - the
following question remains:</p>
<blockquote>
<p>What values of <span class="math inline">\(\beta_0\)</span> and <span
class="math inline">\(\beta_1\)</span> would pick a line in a plane
spawned by <span class="math inline">\(X\)</span> and <span
class="math inline">\(Y\)</span> values so that it describes the assumed
linear relationship between them the best?</p>
</blockquote>
<p>Again:</p>
<pre class="r"><code># Let&#39;s test the assumption of linearity:
ggplot(iris, aes(x = Sepal.Length, y = Petal.Length)) +
  geom_point(size = 2, color = &#39;black&#39;) +
  geom_point(size = 1.5, color = &#39;white&#39;) +
  geom_smooth(method = &#39;lm&#39;, linewidth = .25, color = &#39;red&#39;, se = F) +
  ggtitle(&#39;Sepal Length vs Petal Length&#39;) +
  theme_bw() + 
  theme(panel.border = element_blank())</code></pre>
<p><img src="s07_Correlation_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<p>The line in this <code>{ggplot2}</code> scatter plot is the best
fitting line for the assumed linear relationship between
<code>iris$Sepal.Length</code> (taken to be a predictor, X-axis) and
<code>iris$Petal.Length</code> (taken to be a criterion, Y-axis).
{ggplot2} computed the best fitting line for us: <strong>how?</strong>
Well, in the end, it did it by selecting the <em>optimal</em> values for
<span class="math inline">\(\beta_0\)</span> and <span
class="math inline">\(\beta_1\)</span>. It is our task in this and the
following sessions to figure out how does that selection of optimal
parameter values takes place.</p>
</div>
<div id="simple-linear-regression-w.-lm-in-r" class="section level4">
<h4>3.3 Simple Linear Regression w. <code>lm()</code> in R</h4>
<p>In R we have the <code>lm()</code> function - short for <em>linear
model</em> - to fit all different kinds of models in the scope of this
model framework to the data:</p>
<pre class="r"><code>### --- Linear Regression with lm()
# Predicting: Petal Length from Sepal Length
reg &lt;- lm(Petal.Length ~ Sepal.Length, 
          data = iris) 
class(reg)</code></pre>
<pre><code>## [1] &quot;lm&quot;</code></pre>
<p>The <code>Petal.Length ~ Sepal.Length</code> is called a
<em>formula</em>, and you should learn more about how formulas in R are
syntactically composed. The simplest possible formula, like this one,
simply informs R that we wish to model <code>Petal.Length</code> -
standing to the left of <code>~</code> - by taking only
<code>Sepal.Length</code> - standing to the right - as a predictor. For
those who are already familiar with a multiple linear regression
setting, doing <code>A ~ B + C</code> means calling for a linear model
with <code>A</code> as a dependent variable and <code>B</code>,
<code>C</code> as predictors. We will let these things complicate in the
future, don’t worry. The object <code>reg</code> stores the results of
our attempt at a simple linear regression model, and has its own class
of <code>lm</code>, as you can observe following the call to
<code>class(reg)</code>.</p>
<p>Let’s inspect the result more thoroughly:</p>
<pre class="r"><code>summary(reg)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Petal.Length ~ Sepal.Length, data = iris)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.47747 -0.59072 -0.00668  0.60484  2.49512 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  -7.10144    0.50666  -14.02   &lt;2e-16 ***
## Sepal.Length  1.85843    0.08586   21.65   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.8678 on 148 degrees of freedom
## Multiple R-squared:   0.76,  Adjusted R-squared:  0.7583 
## F-statistic: 468.6 on 1 and 148 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>The output provides:</p>
<ul>
<li>a call that has generated the linear model object
<code>reg</code>;</li>
<li>a basic overview of descriptive statistics for model residuals;</li>
<li>a table of regression coefficients: there are only two for the
simple linear regression model, namely the model intercept and the slope
(i.e. the regression coefficient for the one and only predictor in the
model), and the raw (not standardized) values of the predictors are
reported in the <code>Estimate</code> column, accompanied by respective
standard errors, t-test against zero, and the probabilities of
committing to a <span class="math inline">\(Type I\)</span> error in
drawing conclusions from these t-tests;</li>
<li>Residual Standard Error;</li>
<li>Multiple <span class="math inline">\(R^2\)</span> and the Adjusted
<span class="math inline">\(R^2\)</span> values;</li>
<li>The <span class="math inline">\(F\)</span> test: ratio of variances
computed from the <em>regression</em> and <em>residual SSEs</em>, with
the respective number of degrees of freedom and its p-value.</li>
</ul>
<p>To isolate the regression coefficients from the model:</p>
<pre class="r"><code>coefsReg &lt;- coefficients(reg)
coefsReg</code></pre>
<pre><code>##  (Intercept) Sepal.Length 
##    -7.101443     1.858433</code></pre>
<pre class="r"><code>slopeReg &lt;- coefsReg[2]
print(paste0(&quot;Slope: &quot;, slopeReg))</code></pre>
<pre><code>## [1] &quot;Slope: 1.85843297825484&quot;</code></pre>
<pre class="r"><code>interceptReg &lt;- coefsReg[1]
print(paste0(&quot;Intercept: &quot;, interceptReg))</code></pre>
<pre><code>## [1] &quot;Intercept: -7.10144336960245&quot;</code></pre>
<p>You can also work with the <code>summary()</code> of the
<code>lm</code> class as an object:</p>
<pre class="r"><code>sReg &lt;- summary(reg)
str(sReg)</code></pre>
<pre><code>## List of 11
##  $ call         : language lm(formula = Petal.Length ~ Sepal.Length, data = iris)
##  $ terms        :Classes &#39;terms&#39;, &#39;formula&#39;  language Petal.Length ~ Sepal.Length
##   .. ..- attr(*, &quot;variables&quot;)= language list(Petal.Length, Sepal.Length)
##   .. ..- attr(*, &quot;factors&quot;)= int [1:2, 1] 0 1
##   .. .. ..- attr(*, &quot;dimnames&quot;)=List of 2
##   .. .. .. ..$ : chr [1:2] &quot;Petal.Length&quot; &quot;Sepal.Length&quot;
##   .. .. .. ..$ : chr &quot;Sepal.Length&quot;
##   .. ..- attr(*, &quot;term.labels&quot;)= chr &quot;Sepal.Length&quot;
##   .. ..- attr(*, &quot;order&quot;)= int 1
##   .. ..- attr(*, &quot;intercept&quot;)= int 1
##   .. ..- attr(*, &quot;response&quot;)= int 1
##   .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_GlobalEnv&gt; 
##   .. ..- attr(*, &quot;predvars&quot;)= language list(Petal.Length, Sepal.Length)
##   .. ..- attr(*, &quot;dataClasses&quot;)= Named chr [1:2] &quot;numeric&quot; &quot;numeric&quot;
##   .. .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;Petal.Length&quot; &quot;Sepal.Length&quot;
##  $ residuals    : Named num [1:150] -0.9766 -0.6049 -0.3332 0.0527 -0.7907 ...
##   ..- attr(*, &quot;names&quot;)= chr [1:150] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ...
##  $ coefficients : num [1:2, 1:4] -7.1014 1.8584 0.5067 0.0859 -14.0161 ...
##   ..- attr(*, &quot;dimnames&quot;)=List of 2
##   .. ..$ : chr [1:2] &quot;(Intercept)&quot; &quot;Sepal.Length&quot;
##   .. ..$ : chr [1:4] &quot;Estimate&quot; &quot;Std. Error&quot; &quot;t value&quot; &quot;Pr(&gt;|t|)&quot;
##  $ aliased      : Named logi [1:2] FALSE FALSE
##   ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;(Intercept)&quot; &quot;Sepal.Length&quot;
##  $ sigma        : num 0.868
##  $ df           : int [1:3] 2 148 2
##  $ r.squared    : num 0.76
##  $ adj.r.squared: num 0.758
##  $ fstatistic   : Named num [1:3] 469 1 148
##   ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;value&quot; &quot;numdf&quot; &quot;dendf&quot;
##  $ cov.unscaled : num [1:2, 1:2] 0.34087 -0.05719 -0.05719 0.00979
##   ..- attr(*, &quot;dimnames&quot;)=List of 2
##   .. ..$ : chr [1:2] &quot;(Intercept)&quot; &quot;Sepal.Length&quot;
##   .. ..$ : chr [1:2] &quot;(Intercept)&quot; &quot;Sepal.Length&quot;
##  - attr(*, &quot;class&quot;)= chr &quot;summary.lm&quot;</code></pre>
<p>For example:</p>
<pre class="r"><code>sReg$r.squared</code></pre>
<pre><code>## [1] 0.7599546</code></pre>
<p>Correlation is then:</p>
<pre class="r"><code>sqrt(sReg$r.squared)</code></pre>
<pre><code>## [1] 0.8717538</code></pre>
<pre class="r"><code>sReg$fstatistic</code></pre>
<pre><code>##    value    numdf    dendf 
## 468.5502   1.0000 148.0000</code></pre>
<pre class="r"><code>sReg$coefficients</code></pre>
<pre><code>##               Estimate Std. Error   t value     Pr(&gt;|t|)
## (Intercept)  -7.101443 0.50666229 -14.01613 6.133586e-29
## Sepal.Length  1.858433 0.08585565  21.64602 1.038667e-47</code></pre>
<p>Now, the distribution of residuals - the <span
class="math inline">\(\epsilon\)</span> in the model equation - to be
discussed in the Session:</p>
<pre class="r"><code>hist(sReg$residuals, 20, probability = T,
     main = &#39;Residuals&#39;,
     xlab = &#39;Residuals&#39;, ylab = &#39;Density&#39;,
     col = &quot;orange&quot;)
densRegRes &lt;- data.frame(x  = sReg$residuals,
                         y = dnorm(sReg$residuals, 
                                   mean(sReg$residuals), 
                                   sd(sReg$residuals)))
densRegRes &lt;- densRegRes[order(densRegRes$x), ]
lines(densRegRes,
      lty = &quot;dashed&quot;, 
      lwd = 1, 
      col = &quot;blue&quot;)</code></pre>
<p><img src="s07_Correlation_files/figure-html/unnamed-chunk-37-1.png" width="672" /></p>
<p>Interesting: from the last histogram, would you say that the
residuals in this linear model are <em>normally distributed</em>?</p>
</div>
</div>
<div id="further-readings" class="section level3">
<h3>Further Readings</h3>
<ul>
<li><a
href="https://www.khanacademy.org/math/statistics-probability/describing-relationships-quantitative-data">VIDEO
- In depth, highly recommended: Exploring bivariate numerical data, Khan
Academy</a></li>
<li><a
href="http://www.sthda.com/english/articles/40-regression-analysis/167-simple-linear-regression-in-r/">Simple
Linear Regression in R from STHDA - A brief overview</a></li>
</ul>
</div>
<div id="r-markdown" class="section level3">
<h3>R Markdown</h3>
<p><a href="https://rmarkdown.rstudio.com/">R Markdown</a> is what I
have used to produce this beautiful Notebook. We will learn more about
it near the end of the course, but if you already feel ready to dive
deep, here’s a book: <a href="https://bookdown.org/yihui/rmarkdown/">R
Markdown: The Definitive Guide, Yihui Xie, J. J. Allaire, Garrett
Grolemunds.</a></p>
<hr />
<p>License: <a href="http://www.gnu.org/licenses/gpl-3.0.txt">GPLv3</a>
This Notebook is free software: you can redistribute it and/or modify it
under the terms of the GNU General Public License as published by the
Free Software Foundation, either version 3 of the License, or (at your
option) any later version. This Notebook is distributed in the hope that
it will be useful, but WITHOUT ANY WARRANTY; without even the implied
warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
GNU General Public License for more details. You should have received a
copy of the GNU General Public License along with this Notebook. If not,
see <a href="http://www.gnu.org/licenses/"
class="uri">http://www.gnu.org/licenses/</a>.</p>
<hr />
</div>
</div>

<hr>
<p style="text-align:center;">
  <img src="DK_Logo_White_150.png">
</p>
<p style="font-size:13px;text-align:center;">
  <b>Contact: </b><a href = "mailto: goran.milovanovic@datakolektiv.com">goran.milovanovic@datakolektiv.com</a><br><br><a href="https://github.com/datakolektiv"><img src="github.png"></a>&nbsp;&nbsp;<a href="https://www.linkedin.com/in/gmilovanovic/"><img src="linkedin.png"></a>
</p>
<p style="font-size:11px;text-align:center;"><b>Impressum</b><br>
<b>Data Kolektiv, 2004, Belgrade.</b></p>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3,h4,h5",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
